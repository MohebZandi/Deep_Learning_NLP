{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Book_Deep_Learning_NLP_Jason Brownlee_2020_Foundation_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6urCpl2sQM8vroW380Gs3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohebZandi/Deep_Learning_NLP/blob/main/2_Book_Deep_Learning_NLP_Jason_Brownlee_2020_Foundation_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Book_Deep_Learning_NLP_Jason Brownlee_2020**\n",
        "\n",
        "# Section Two: \n",
        "Jason Brownlee\n",
        "2020"
      ],
      "metadata": {
        "id": "ZGG80QwzU7DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part VII Language Modeling"
      ],
      "metadata": {
        "id": "AWGj1_MXVM8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Language Modeling**\n",
        "\n",
        "Language modeling is central to many important natural language processing tasks. \n",
        "\n",
        "Recently, neural-network-based language models have demonstrated better performance than classical methods both standalone and as part of more challenging natural language processing tasks. \n",
        "\n",
        "In this chapter, you will discover language modeling for natural language processing."
      ],
      "metadata": {
        "id": "TaxEeZhjVYBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial is divided into the following parts:\n",
        "1. Problem of Modeling Language\n",
        "2. Statistical Language Modeling\n",
        "3. Neural Language Models\n",
        "\n",
        "**Problem of Modeling Language**\n",
        "\n",
        "Formal languages, like programming languages, can be fully specified. \n",
        "\n",
        "All the reserved words can be defined and the valid ways that they can be used can be precisely defined. \n",
        "\n",
        "We cannot do this with natural language. Natural languages are not designed; they emerge, and therefore there is no formal specification.\n",
        "\n",
        "**Statistical Language Modeling**\n",
        "\n",
        "Statistical Language Modeling, or Language Modeling and LM for short, is the development of probabilistic models that are able to predict the next word in the sequence given the words that precede it.\n",
        "\n",
        "A language model learns the probability of word occurrence based on examples of text.\n",
        "\n",
        "Simpler models may look at a context of a short sequence of words, whereas larger models may work at the level of sentences or paragraphs. \n",
        "\n",
        "Most commonly, language models operate at the level of words.\n",
        "\n",
        "A language model is a function that puts a probability measure over strings drawn from some vocabulary.\n",
        "\n",
        "**Language modeling is a crucial component in real-world applications such as machine-translation and automatic speech recognition.**\n",
        "\n",
        "For these reasons, language modeling plays a central role in natural-language processing, AI, and machine learning research."
      ],
      "metadata": {
        "id": "FjkgpXQEVuFv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good example is speech recognition, where audio data is used as an input to the model and the output requires a language model that interprets the input signal and recognizes each new word within the context of the words already recognized.\n",
        "\n",
        "Similarly, language models are used to generate text in many similar natural language processing tasks, for example:\n",
        "- Optical Character Recognition\n",
        "- Handwriting Recognition.\n",
        "- Machine Translation.\n",
        "- Spelling Correction.\n",
        "- Image Captioning.\n",
        "- Text Summarization\n",
        "- And much more.\n",
        "\n",
        "**Neural Language Models**\n",
        "\n",
        "Recently, the use of neural networks in the development of language models has become very popular, to the point that it may now be the preferred approach.\n",
        "\n",
        "The use of neural networks in language modeling is often called **Neural Language Modeling**, or **NLM** for short.\n",
        "\n"
      ],
      "metadata": {
        "id": "Oq-lMwDpXtUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to Develop a Character-Based Neural Language Model**\n",
        "\n",
        "A language model predicts the next word in the sequence based on the specific words that have come before it in the sequence. \n",
        "\n",
        "It is also possible to develop language models at the character\n",
        "level using neural networks. \n",
        "\n",
        "The benefit of character-based language models is their small vocabulary and \n",
        "exibility in handling any words, punctuation, and other document structure.\n",
        "\n",
        "In this tutorial, you will discover how to\n",
        "develop a character-based neural language model. \n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "- How to prepare text for character-based language modeling.\n",
        "- How to develop a character-based language model using LSTMs.\n",
        "- How to use a trained character-based language model to generate text.\n",
        "\n",
        "This tutorial is divided into the following parts:\n",
        "1. Sing a Song of Sixpence\n",
        "2. Data Preparation\n",
        "3. Train Language Model\n",
        "4. Generate Text"
      ],
      "metadata": {
        "id": "yQak-PfYnqwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sing a Song of Sixpence**\n",
        "\n",
        "Sing a song of sixpence,\n",
        "\n",
        "A pocket full of rye.\n",
        "\n",
        "Four and twenty blackbirds,\n",
        "\n",
        "Baked in a pie.\n",
        "\n",
        "When the pie was opened\n",
        "\n",
        "The birds began to sing;\n",
        "\n",
        "Wasn't that a dainty dish,\n",
        "\n",
        "To set before the king.\n",
        "\n",
        "The king was in his counting house,\n",
        "\n",
        "Counting out his money;\n",
        "\n",
        "The queen was in the parlour,\n",
        "\n",
        "Eating bread and honey.\n",
        "\n",
        "The maid was in the garden,\n",
        "\n",
        "Hanging out the clothes,\n",
        "\n",
        "When down came a blackbird\n",
        "\n",
        "And pecked off her nose."
      ],
      "metadata": {
        "id": "-eeWGkndanot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation**\n",
        "\n",
        "The first step is to prepare the text data. We will start by defining the type of language model.\n",
        "\n",
        "A language model must be trained on the text, and in the case of a character-based language model, the input and output sequences must be characters. \n",
        "\n",
        "The number of characters used as input will also define the number of characters that will need to be provided to the model in order to elicit the first predicted character. \n",
        "\n",
        "After the first character has been generated, it can be appended to the input sequence and used as input for the model to generate the next character.\n",
        "\n",
        "**Load Text**\n",
        "\n",
        "function named *load doc()*\n",
        "\n",
        "raw_text = load_doc('rhyme.txt')\n",
        "\n",
        "print(raw_text)\n",
        "\n",
        "**Clean Text**\n",
        "\n",
        "Next, we need to clean the loaded text. We will not do much to it on this example. Speciffically, we will strip all of the new line characters so that we have one long sequence of characters separated only by white space.\n",
        "\n",
        "**Create Sequences**\n",
        "\n",
        "Now that we have a long list of characters, we can create our input-output sequences used to train the model. \n",
        "\n",
        "Each input sequence will be 10 characters with one output character, making\n",
        "each sequence 11 characters long. \n",
        "\n",
        "We can create the sequences by enumerating the characters in the text, starting at the 11th character at index 10.\n",
        "\n",
        "**Save Sequences**\n",
        "\n",
        "function *save_doc()*\n",
        "\n",
        "out_filename = 'char_sequences.txt'\n",
        "\n",
        "save_doc(sequences, out_filename)\n",
        "\n",
        "**Complete Example**\n",
        "\n",
        "Tying all of this together, the complete code listing is provided below."
      ],
      "metadata": {
        "id": "LCjL9jBTbV_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmtafm-X48b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2d3df3-6ab7-4c29-f4b8-c9bdc3200bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Because there are lots of files to be read in memory, I have uploaded the folders in \n",
        "# Google Drive, so I have to mount it first, then read the data\n",
        "\n",
        "# In this method the Authentication of google drive will open a new window.\n",
        "# There are also other ways to inform the Auth to google in program text\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')     # Mounting Google Drive in Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "# load text\n",
        "raw_text = load_doc('/content/gdrive/My Drive/txt_sentoken//rhyme.txt')\n",
        "print('raw_text:\\n\\n',raw_text)\n",
        "\n",
        "# clean\n",
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "# organize into sequences of characters\n",
        "length = 10\n",
        "sequences = list()\n",
        "\n",
        "for i in range(length, len(raw_text)):\n",
        "    # select sequence of tokens\n",
        "    seq = raw_text[i-length:i+1]\n",
        "    # store\n",
        "    sequences.append(seq)\n",
        "\n",
        "print('\\n\\nTotal Sequences: %d' % len(sequences))\n",
        "# save sequences to file\n",
        "out_filename = '/content/gdrive/My Drive/txt_sentoken/char_sequences.txt'\n",
        "print('\\n\\nSample of Sequences:\\n',sequences[:5])\n",
        "save_doc(sequences, out_filename)"
      ],
      "metadata": {
        "id": "UIujM7q45Zb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3622fe3a-29af-4faf-9692-8b9dbe11a98e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw_text:\n",
            "\n",
            " Sing a song of sixpence,\n",
            "A pocket full of rye.\n",
            "Four and twenty blackbirds,\n",
            "Baked in a pie.\n",
            "When the pie was opened\n",
            "The birds began to sing;\n",
            "Wasn't that a dainty dish,\n",
            "To set before the king.\n",
            "The king was in his counting house,\n",
            "Counting out his money;\n",
            "The queen was in the parlour,\n",
            "Eating bread and honey.\n",
            "The maid was in the garden,\n",
            "Hanging out the clothes,\n",
            "When down came a blackbird\n",
            "And pecked off her nose.\n",
            "\n",
            "\n",
            "Total Sequences: 399\n",
            "\n",
            "\n",
            "Sample of Sequences:\n",
            " ['Sing a song', 'ing a song ', 'ng a song o', 'g a song of', ' a song of ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to train our character-based neural language model.\n",
        "\n",
        "**Load Data**\n",
        "\n",
        "Calling *load_doc()*\n",
        "\n",
        "in_filename = 'char_sequences.txt'\n",
        "\n",
        "raw_text = load_doc(in_filename)\n",
        "\n",
        "lines = raw_text.split('\\n')\n",
        "\n",
        "**Encode Sequences**\n",
        "\n",
        "The sequences of characters must be encoded as integers. This means that each unique character will be assigned a specific integer value and each sequence of characters will be encoded as a sequence of integers. \n",
        "\n",
        "We can create the mapping given a sorted set of unique characters in the\n",
        "raw input data. The mapping is a dictionary of character values to integer values.\n",
        "\n",
        "\n",
        "**Split Inputs and Output**\n",
        "\n",
        "Now that the sequences have been integer encoded, we can separate the columns into input and output sequences of characters. We can do this using a simple array slice.\n",
        "\n",
        "sequences = array(sequences)\n",
        "\n",
        "X, y = sequences[: , :-1], sequences[: , -1]\n",
        "\n",
        "**Fit Model**\n",
        "\n",
        "The model is defined with an input layer that takes sequences that have 10 time steps and 38 features for the one hot encoded input sequences. \n",
        "\n",
        "Rather than specify these numbers, we use the second and third dimensions on the X input data. \n",
        "\n",
        "This is so that if we change the length of the sequences or size of the vocabulary, we do not need to change the model definition. \n",
        "\n",
        "The model has a single LSTM hidden layer with 75 memory cells, chosen with a little trial and error. \n",
        "\n",
        "The model has a fully connected output layer that outputs one vector with a probability distribution across all characters in the vocabulary. \n",
        "\n",
        "A softmax activation function is used on the output layer to ensure the output has the properties of a probability distribution.\n",
        "\n",
        "The model is learning a multiclass classiffication problem, therefore we use the categorical log loss intended for this type of problem. \n",
        "\n",
        "The efficient Adam implementation of gradient descent is used to optimize the model and accuracy is reported at the end of each batch update.\n",
        "\n",
        "**Save Model**\n",
        "\n",
        "model.save('model_2.h5')\n",
        "\n",
        "**Complete Example**"
      ],
      "metadata": {
        "id": "kLK_O6jeiJN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Sequences\n",
        "\n",
        "chars = sorted(list(set(raw_text)))\n",
        "\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "\n",
        "sequences = list()\n",
        "for line in lines:\n",
        "    # integer encode line\n",
        "    encoded_seq = [mapping[char] for char in line]\n",
        "    # store\n",
        "    sequences.append(encoded_seq)"
      ],
      "metadata": {
        "id": "g1-zTRwC5Zex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# define the model\n",
        "def define_model(X):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    # compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize defined model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model\n",
        "\n",
        "# load\n",
        "in_filename = '/content/gdrive/My Drive/txt_sentoken/char_sequences.txt'\n",
        "raw_text = load_doc(in_filename)\n",
        "lines = raw_text.split('\\n')\n",
        "\n",
        "# integer encode sequences of characters\n",
        "chars = sorted(list(set(raw_text)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "sequences = list()\n",
        "for line in lines:\n",
        "    # integer encode line\n",
        "    encoded_seq = [mapping[char] for char in line]\n",
        "    # store\n",
        "    sequences.append(encoded_seq)\n",
        "\n",
        "# vocabulary size\n",
        "vocab_size = len(mapping)\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(X)\n",
        "model.pre\n",
        "# fit model\n",
        "model.fit(X, y, epochs=100, verbose=2)\n",
        "# save the model to file\n",
        "model.save('/content/gdrive/My Drive/txt_sentoken/model_2.h5')\n",
        "# save the mapping\n",
        "dump(mapping, open('/content/gdrive/My Drive/txt_sentoken/mapping.pkl', 'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSFS7yjtoJeR",
        "outputId": "c48e2a7d-5245-42ae-8738-c5a69d253e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 38\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 75)                34200     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 38)                2888      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37,088\n",
            "Trainable params: 37,088\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "13/13 - 2s - loss: 3.6144 - accuracy: 0.0627 - 2s/epoch - 145ms/step\n",
            "Epoch 2/100\n",
            "13/13 - 0s - loss: 3.4974 - accuracy: 0.1654 - 82ms/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "13/13 - 0s - loss: 3.1485 - accuracy: 0.1905 - 86ms/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "13/13 - 0s - loss: 3.0611 - accuracy: 0.1905 - 99ms/epoch - 8ms/step\n",
            "Epoch 5/100\n",
            "13/13 - 0s - loss: 3.0089 - accuracy: 0.1905 - 100ms/epoch - 8ms/step\n",
            "Epoch 6/100\n",
            "13/13 - 0s - loss: 2.9897 - accuracy: 0.1905 - 87ms/epoch - 7ms/step\n",
            "Epoch 7/100\n",
            "13/13 - 0s - loss: 2.9763 - accuracy: 0.1905 - 87ms/epoch - 7ms/step\n",
            "Epoch 8/100\n",
            "13/13 - 0s - loss: 2.9672 - accuracy: 0.1905 - 90ms/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "13/13 - 0s - loss: 2.9564 - accuracy: 0.1905 - 88ms/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "13/13 - 0s - loss: 2.9513 - accuracy: 0.1905 - 92ms/epoch - 7ms/step\n",
            "Epoch 11/100\n",
            "13/13 - 0s - loss: 2.9305 - accuracy: 0.1905 - 90ms/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "13/13 - 0s - loss: 2.9153 - accuracy: 0.1905 - 94ms/epoch - 7ms/step\n",
            "Epoch 13/100\n",
            "13/13 - 0s - loss: 2.8899 - accuracy: 0.1905 - 83ms/epoch - 6ms/step\n",
            "Epoch 14/100\n",
            "13/13 - 0s - loss: 2.8701 - accuracy: 0.1980 - 86ms/epoch - 7ms/step\n",
            "Epoch 15/100\n",
            "13/13 - 0s - loss: 2.8434 - accuracy: 0.2055 - 92ms/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "13/13 - 0s - loss: 2.8182 - accuracy: 0.2105 - 100ms/epoch - 8ms/step\n",
            "Epoch 17/100\n",
            "13/13 - 0s - loss: 2.7886 - accuracy: 0.2331 - 103ms/epoch - 8ms/step\n",
            "Epoch 18/100\n",
            "13/13 - 0s - loss: 2.7491 - accuracy: 0.2206 - 82ms/epoch - 6ms/step\n",
            "Epoch 19/100\n",
            "13/13 - 0s - loss: 2.7036 - accuracy: 0.2406 - 90ms/epoch - 7ms/step\n",
            "Epoch 20/100\n",
            "13/13 - 0s - loss: 2.6648 - accuracy: 0.2732 - 107ms/epoch - 8ms/step\n",
            "Epoch 21/100\n",
            "13/13 - 0s - loss: 2.6218 - accuracy: 0.2807 - 87ms/epoch - 7ms/step\n",
            "Epoch 22/100\n",
            "13/13 - 0s - loss: 2.5615 - accuracy: 0.2682 - 90ms/epoch - 7ms/step\n",
            "Epoch 23/100\n",
            "13/13 - 0s - loss: 2.5150 - accuracy: 0.3008 - 90ms/epoch - 7ms/step\n",
            "Epoch 24/100\n",
            "13/13 - 0s - loss: 2.4816 - accuracy: 0.3108 - 95ms/epoch - 7ms/step\n",
            "Epoch 25/100\n",
            "13/13 - 0s - loss: 2.4213 - accuracy: 0.3233 - 86ms/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "13/13 - 0s - loss: 2.3635 - accuracy: 0.3484 - 93ms/epoch - 7ms/step\n",
            "Epoch 27/100\n",
            "13/13 - 0s - loss: 2.3264 - accuracy: 0.3258 - 88ms/epoch - 7ms/step\n",
            "Epoch 28/100\n",
            "13/13 - 0s - loss: 2.2677 - accuracy: 0.3609 - 89ms/epoch - 7ms/step\n",
            "Epoch 29/100\n",
            "13/13 - 0s - loss: 2.2174 - accuracy: 0.3659 - 87ms/epoch - 7ms/step\n",
            "Epoch 30/100\n",
            "13/13 - 0s - loss: 2.1826 - accuracy: 0.3835 - 88ms/epoch - 7ms/step\n",
            "Epoch 31/100\n",
            "13/13 - 0s - loss: 2.1506 - accuracy: 0.3960 - 91ms/epoch - 7ms/step\n",
            "Epoch 32/100\n",
            "13/13 - 0s - loss: 2.0825 - accuracy: 0.4336 - 92ms/epoch - 7ms/step\n",
            "Epoch 33/100\n",
            "13/13 - 0s - loss: 2.0404 - accuracy: 0.4236 - 91ms/epoch - 7ms/step\n",
            "Epoch 34/100\n",
            "13/13 - 0s - loss: 1.9896 - accuracy: 0.4236 - 86ms/epoch - 7ms/step\n",
            "Epoch 35/100\n",
            "13/13 - 0s - loss: 1.9744 - accuracy: 0.4461 - 93ms/epoch - 7ms/step\n",
            "Epoch 36/100\n",
            "13/13 - 0s - loss: 1.9324 - accuracy: 0.4461 - 90ms/epoch - 7ms/step\n",
            "Epoch 37/100\n",
            "13/13 - 0s - loss: 1.8822 - accuracy: 0.4787 - 106ms/epoch - 8ms/step\n",
            "Epoch 38/100\n",
            "13/13 - 0s - loss: 1.8492 - accuracy: 0.4787 - 111ms/epoch - 9ms/step\n",
            "Epoch 39/100\n",
            "13/13 - 0s - loss: 1.7830 - accuracy: 0.5163 - 90ms/epoch - 7ms/step\n",
            "Epoch 40/100\n",
            "13/13 - 0s - loss: 1.7424 - accuracy: 0.5088 - 93ms/epoch - 7ms/step\n",
            "Epoch 41/100\n",
            "13/13 - 0s - loss: 1.7131 - accuracy: 0.5263 - 93ms/epoch - 7ms/step\n",
            "Epoch 42/100\n",
            "13/13 - 0s - loss: 1.6798 - accuracy: 0.5338 - 94ms/epoch - 7ms/step\n",
            "Epoch 43/100\n",
            "13/13 - 0s - loss: 1.6297 - accuracy: 0.5388 - 91ms/epoch - 7ms/step\n",
            "Epoch 44/100\n",
            "13/13 - 0s - loss: 1.5953 - accuracy: 0.5739 - 96ms/epoch - 7ms/step\n",
            "Epoch 45/100\n",
            "13/13 - 0s - loss: 1.5532 - accuracy: 0.5564 - 110ms/epoch - 8ms/step\n",
            "Epoch 46/100\n",
            "13/13 - 0s - loss: 1.5153 - accuracy: 0.6140 - 109ms/epoch - 8ms/step\n",
            "Epoch 47/100\n",
            "13/13 - 0s - loss: 1.4739 - accuracy: 0.5965 - 102ms/epoch - 8ms/step\n",
            "Epoch 48/100\n",
            "13/13 - 0s - loss: 1.4343 - accuracy: 0.6366 - 92ms/epoch - 7ms/step\n",
            "Epoch 49/100\n",
            "13/13 - 0s - loss: 1.4048 - accuracy: 0.6165 - 88ms/epoch - 7ms/step\n",
            "Epoch 50/100\n",
            "13/13 - 0s - loss: 1.3559 - accuracy: 0.6341 - 96ms/epoch - 7ms/step\n",
            "Epoch 51/100\n",
            "13/13 - 0s - loss: 1.3358 - accuracy: 0.6466 - 89ms/epoch - 7ms/step\n",
            "Epoch 52/100\n",
            "13/13 - 0s - loss: 1.2937 - accuracy: 0.6591 - 94ms/epoch - 7ms/step\n",
            "Epoch 53/100\n",
            "13/13 - 0s - loss: 1.2606 - accuracy: 0.6767 - 87ms/epoch - 7ms/step\n",
            "Epoch 54/100\n",
            "13/13 - 0s - loss: 1.2235 - accuracy: 0.6767 - 88ms/epoch - 7ms/step\n",
            "Epoch 55/100\n",
            "13/13 - 0s - loss: 1.1961 - accuracy: 0.7043 - 96ms/epoch - 7ms/step\n",
            "Epoch 56/100\n",
            "13/13 - 0s - loss: 1.1412 - accuracy: 0.7368 - 98ms/epoch - 8ms/step\n",
            "Epoch 57/100\n",
            "13/13 - 0s - loss: 1.1081 - accuracy: 0.7118 - 98ms/epoch - 8ms/step\n",
            "Epoch 58/100\n",
            "13/13 - 0s - loss: 1.0826 - accuracy: 0.7444 - 90ms/epoch - 7ms/step\n",
            "Epoch 59/100\n",
            "13/13 - 0s - loss: 1.0443 - accuracy: 0.7569 - 90ms/epoch - 7ms/step\n",
            "Epoch 60/100\n",
            "13/13 - 0s - loss: 1.0147 - accuracy: 0.7569 - 94ms/epoch - 7ms/step\n",
            "Epoch 61/100\n",
            "13/13 - 0s - loss: 0.9759 - accuracy: 0.8020 - 87ms/epoch - 7ms/step\n",
            "Epoch 62/100\n",
            "13/13 - 0s - loss: 0.9528 - accuracy: 0.7895 - 89ms/epoch - 7ms/step\n",
            "Epoch 63/100\n",
            "13/13 - 0s - loss: 0.9086 - accuracy: 0.8170 - 87ms/epoch - 7ms/step\n",
            "Epoch 64/100\n",
            "13/13 - 0s - loss: 0.8764 - accuracy: 0.8095 - 85ms/epoch - 7ms/step\n",
            "Epoch 65/100\n",
            "13/13 - 0s - loss: 0.8569 - accuracy: 0.8246 - 83ms/epoch - 6ms/step\n",
            "Epoch 66/100\n",
            "13/13 - 0s - loss: 0.8231 - accuracy: 0.8371 - 88ms/epoch - 7ms/step\n",
            "Epoch 67/100\n",
            "13/13 - 0s - loss: 0.7934 - accuracy: 0.8521 - 83ms/epoch - 6ms/step\n",
            "Epoch 68/100\n",
            "13/13 - 0s - loss: 0.7752 - accuracy: 0.8471 - 100ms/epoch - 8ms/step\n",
            "Epoch 69/100\n",
            "13/13 - 0s - loss: 0.7371 - accuracy: 0.8822 - 88ms/epoch - 7ms/step\n",
            "Epoch 70/100\n",
            "13/13 - 0s - loss: 0.7035 - accuracy: 0.8822 - 85ms/epoch - 7ms/step\n",
            "Epoch 71/100\n",
            "13/13 - 0s - loss: 0.6919 - accuracy: 0.8797 - 85ms/epoch - 7ms/step\n",
            "Epoch 72/100\n",
            "13/13 - 0s - loss: 0.6521 - accuracy: 0.9198 - 86ms/epoch - 7ms/step\n",
            "Epoch 73/100\n",
            "13/13 - 0s - loss: 0.6432 - accuracy: 0.9223 - 92ms/epoch - 7ms/step\n",
            "Epoch 74/100\n",
            "13/13 - 0s - loss: 0.6079 - accuracy: 0.9173 - 86ms/epoch - 7ms/step\n",
            "Epoch 75/100\n",
            "13/13 - 0s - loss: 0.6010 - accuracy: 0.9073 - 87ms/epoch - 7ms/step\n",
            "Epoch 76/100\n",
            "13/13 - 0s - loss: 0.5794 - accuracy: 0.9298 - 92ms/epoch - 7ms/step\n",
            "Epoch 77/100\n",
            "13/13 - 0s - loss: 0.5597 - accuracy: 0.9273 - 88ms/epoch - 7ms/step\n",
            "Epoch 78/100\n",
            "13/13 - 0s - loss: 0.5291 - accuracy: 0.9449 - 86ms/epoch - 7ms/step\n",
            "Epoch 79/100\n",
            "13/13 - 0s - loss: 0.5066 - accuracy: 0.9424 - 111ms/epoch - 9ms/step\n",
            "Epoch 80/100\n",
            "13/13 - 0s - loss: 0.4827 - accuracy: 0.9649 - 95ms/epoch - 7ms/step\n",
            "Epoch 81/100\n",
            "13/13 - 0s - loss: 0.4662 - accuracy: 0.9624 - 81ms/epoch - 6ms/step\n",
            "Epoch 82/100\n",
            "13/13 - 0s - loss: 0.4422 - accuracy: 0.9724 - 92ms/epoch - 7ms/step\n",
            "Epoch 83/100\n",
            "13/13 - 0s - loss: 0.4253 - accuracy: 0.9724 - 92ms/epoch - 7ms/step\n",
            "Epoch 84/100\n",
            "13/13 - 0s - loss: 0.4128 - accuracy: 0.9624 - 90ms/epoch - 7ms/step\n",
            "Epoch 85/100\n",
            "13/13 - 0s - loss: 0.3954 - accuracy: 0.9774 - 93ms/epoch - 7ms/step\n",
            "Epoch 86/100\n",
            "13/13 - 0s - loss: 0.3866 - accuracy: 0.9774 - 88ms/epoch - 7ms/step\n",
            "Epoch 87/100\n",
            "13/13 - 0s - loss: 0.3694 - accuracy: 0.9799 - 89ms/epoch - 7ms/step\n",
            "Epoch 88/100\n",
            "13/13 - 0s - loss: 0.3511 - accuracy: 0.9799 - 87ms/epoch - 7ms/step\n",
            "Epoch 89/100\n",
            "13/13 - 0s - loss: 0.3392 - accuracy: 0.9774 - 94ms/epoch - 7ms/step\n",
            "Epoch 90/100\n",
            "13/13 - 0s - loss: 0.3219 - accuracy: 0.9850 - 102ms/epoch - 8ms/step\n",
            "Epoch 91/100\n",
            "13/13 - 0s - loss: 0.3085 - accuracy: 0.9799 - 90ms/epoch - 7ms/step\n",
            "Epoch 92/100\n",
            "13/13 - 0s - loss: 0.2987 - accuracy: 0.9825 - 85ms/epoch - 7ms/step\n",
            "Epoch 93/100\n",
            "13/13 - 0s - loss: 0.2865 - accuracy: 0.9900 - 91ms/epoch - 7ms/step\n",
            "Epoch 94/100\n",
            "13/13 - 0s - loss: 0.2836 - accuracy: 0.9825 - 90ms/epoch - 7ms/step\n",
            "Epoch 95/100\n",
            "13/13 - 0s - loss: 0.2711 - accuracy: 0.9875 - 89ms/epoch - 7ms/step\n",
            "Epoch 96/100\n",
            "13/13 - 0s - loss: 0.2567 - accuracy: 0.9900 - 96ms/epoch - 7ms/step\n",
            "Epoch 97/100\n",
            "13/13 - 0s - loss: 0.2519 - accuracy: 0.9925 - 86ms/epoch - 7ms/step\n",
            "Epoch 98/100\n",
            "13/13 - 0s - loss: 0.2373 - accuracy: 0.9900 - 88ms/epoch - 7ms/step\n",
            "Epoch 99/100\n",
            "13/13 - 0s - loss: 0.2266 - accuracy: 0.9950 - 88ms/epoch - 7ms/step\n",
            "Epoch 100/100\n",
            "13/13 - 0s - loss: 0.2209 - accuracy: 0.9925 - 90ms/epoch - 7ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Text**\n",
        "\n",
        "We will use the learned language model to generate new sequences of text that have the same statistical properties.\n",
        "\n",
        "The first step is to load the model saved to the file model.h5. We can use the *load_model()* function from the Keras API.\n",
        "\n",
        "We also need to load the pickled dictionary for mapping characters to integers from the file mapping.pkl. We will use the Pickle API to load the object.\n",
        "\n",
        "**Complete Example**\n",
        "\n"
      ],
      "metadata": {
        "id": "nL-D8SCpp7-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of characters\n",
        "    for _ in range(n_chars):\n",
        "        # encode the characters as integers\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # one hot encode\n",
        "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "        # predict character\n",
        "        yhat = model.predict(encoded)\n",
        "        classes_x=np.argmax(yhat,axis=1)\n",
        "\n",
        "        # reverse map integer to character\n",
        "        out_char = ''\n",
        "        # print(yhat)\n",
        "        # print()\n",
        "        # print(classes_x)\n",
        "        for char, index in mapping.items():\n",
        "            if index == classes_x:\n",
        "                out_char = char\n",
        "            break\n",
        "        # append to input\n",
        "        in_text += out_char\n",
        "    return in_text\n",
        "# load the model\n",
        "model = load_model('/content/gdrive/My Drive/txt_sentoken/model_2.h5')\n",
        "# load the mapping\n",
        "mapping = load(open('/content/gdrive/My Drive/txt_sentoken/mapping.pkl', 'rb'))\n",
        "\n",
        "# test start of rhyme\n",
        "print(generate_seq(model, mapping, 10, 'Sing a son', 20))\n",
        "\n",
        "# test mid-line\n",
        "print(generate_seq(model, mapping, 10, 'king was i', 20))\n",
        "\n",
        "# test not in original\n",
        "print(generate_seq(model, mapping, 10, 'hello worl', 20))"
      ],
      "metadata": {
        "id": "xgyRe0fl5Zio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "886f80fc-db85-4e60-b9f1-b1758746b506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sing a son\n",
            "king was i\n",
            "hello worl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to Develop a Word-Based Neural Language Model**"
      ],
      "metadata": {
        "id": "moyMmS8cHx8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "agP5keuv5Zle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "neiAKEES5ZoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N3MHYRP95Zq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bPLOjpxW5Zuc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}