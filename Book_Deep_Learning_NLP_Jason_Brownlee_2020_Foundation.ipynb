{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Book_Deep_Learning_NLP_Jason Brownlee_2020_Foundation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1qJrm-4pGIievAqN2pa1ltkMcPwDCaVSa",
      "authorship_tag": "ABX9TyM7gLG/7w1uU3b9n/JuJ1Oa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohebZandi/Deep_Learning_NLP/blob/main/Book_Deep_Learning_NLP_Jason_Brownlee_2020_Foundation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Book_Deep_Learning_NLP_Jason Brownlee_2020**\n",
        "\n",
        "# Section One: Bag of Words, First models of Neural Network and word Embeding\n",
        "Jason Brownlee\n",
        "2020"
      ],
      "metadata": {
        "id": "XpP-LrpbTU5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II - Foundations\n",
        "### Foundations\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kf082u6TTO4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Chapter 1\n",
        "#### Natural Language Processing\n",
        "\n",
        "What natural language is and how it is different from other types of data.\n",
        "\n",
        "What makes working with natural language so challenging.\n",
        "\n",
        "Where the field of NLP came from and how it is defined by modern practitioners.\n",
        "\n",
        "\n",
        "#### Chapter 2\n",
        "#### Deep Learning\n",
        "\n",
        "-  The motivation for exploring and adopting large neural network models.\n",
        "-  The perspective on deep learning as hierarchical feature learning.\n",
        "-  The promise of scalability of deep learning with the size of data.\n",
        "\n",
        "\n",
        "\n",
        "#### Chapter 3\n",
        "#### Promise of Deep Learning for Natural Language\n",
        "\n",
        "1- The Promise of Drop-in Replacement Models. That is, deep learning methods can\n",
        "be dropped into existing natural language systems as replacement models that can achieve\n",
        "commensurate or better performance.\n",
        "\n",
        "2- The Promise of New NLP Models. That is, deep learning methods offer the op-\n",
        "portunity of new modeling approaches to challenging natural language problems like\n",
        "sequence-to-sequence prediction.\n",
        "\n",
        "3- The Promise of Feature Learning. That is, that deep learning methods can learn\n",
        "the features from natural language required by the model, rather than requiring that the\n",
        "features be specified and extracted by an expert.\n",
        "\n",
        "4- The Promise of Continued Improvement. That is, that the performance of deep\n",
        "learning in natural language processing is based on real results and that the improvements\n",
        "appear to be continuing and perhaps speeding up.\n",
        "\n",
        "5- The Promise of End-to-End Models. That is, that large end-to-end deep learning\n",
        "models can be fit on natural language problems offering a more general and better-\n",
        "performing approach.\n",
        "\n",
        "#### Chapter 4\n",
        "#### How to Develop Deep Learning Models With Keras\n",
        "\n",
        "Keras Model Life-Cycle\n",
        "\n",
        "Below is an overview of the 5 steps in the neural network model life-cycle in Keras:\n",
        "1. Define Network.\n",
        "2. Compile Network.\n",
        "3. Fit Network.\n",
        "4. Evaluate Network.\n",
        "5. Make Predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "HfQvrc5F9Xmk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAwNfwCsSo8t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range = (0, 1))\n",
        "\n",
        "from keras.layers import LSTM, Bidirectional, Dense, Activation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.optimizers import Adam,Nadam, SGD\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define Network\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(2))\n",
        "\n",
        "# or\n",
        "\n",
        "layers = [Dense(2)]\n",
        "model = Sequential(layers)"
      ],
      "metadata": {
        "id": "qPn9FxTSStvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For example, a small Multilayer Perceptron model\n",
        "# with 2 inputs in the visible layer, 5 neurons in the hidden layer and one neuron in the output\n",
        "# layer can be defined as:\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(5, input_dim=2))\n",
        "model.add(Dense(1))\n"
      ],
      "metadata": {
        "id": "SJiB_8cYS8lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation functions that transform a summed signal from each neuron in a layer can be extracted and\n",
        "# added to the Sequential as a layer-like object called the Activation class.\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(5, input_dim=2))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))"
      ],
      "metadata": {
        "id": "WKPhM0LcUhce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of Activation function is most important for the output layer as it will define the format that predictions will take. \n",
        "\n",
        "For example, below are some common predictive modeling\n",
        "problem types and the structure and standard activation function that you can use in the output\n",
        "\n",
        "layer:\n",
        "1- Regression: Linear activation function, or *linear*, and the number of neurons matching\n",
        "the number of outputs.\n",
        "\n",
        "2- Binary Classification (2 class): Logistic activation function, or *sigmoid*, and one\n",
        "neuron the output layer.\n",
        "\n",
        "3- Multiclass Classification (>2 class): Softmax activation function, or *softmax*, and\n",
        "one output neuron per class value, assuming a one hot encoded output pattern."
      ],
      "metadata": {
        "id": "9pPlOTRAViOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Compile Network\n",
        "\n",
        "# Example of compiling a defined model.\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "xLcvcEtMVLoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternately, the optimizer can be created and configured before being provided as an argument\n",
        "# to the compilation step.\n",
        "\n",
        "algorithm = SGD(lr=0.1, momentum=0.3)\n",
        "model.compile(optimizer=algorithm, loss='mean_squared_error')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjQ463SMWVHx",
        "outputId": "40ced94e-bbd6-41a8-dcb5-764d670a1672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are some standard loss functions for different predictive\n",
        "model types:\n",
        "\n",
        "- Regression: Mean Squared Error or *mean_squared_error*.\n",
        "\n",
        "- Binary Classification (2 class): Logarithmic Loss, also called cross-entropy or\n",
        "*binary_crossentropy*.\n",
        "\n",
        "- Multiclass Classification (>2 class): Multiclass Logarithmic Loss or *categorical_crossentropy*."
      ],
      "metadata": {
        "id": "Vd6mpik9X7mS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common optimization algorithm is *stochastic gradient descent(sgd)*, but Keras also\n",
        "supports a suite of other state-of-the-art optimization algorithms that work well with little or\n",
        "no configuration. Perhaps the most commonly used optimization algorithms because of their\n",
        "generally better performance are:\n",
        "\n",
        "- Stochastic Gradient Descent, or *sgd*, that requires the tuning of a learning rate and\n",
        "momentum.\n",
        "\n",
        "- Adam, or *adam*, that requires the tuning of learning rate.\n",
        "\n",
        "- RMSprop, or *rmsprop*, that requires the tuning of learning rate."
      ],
      "metadata": {
        "id": "4ti4TZdSYhRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generally, the most useful additional metric to collect is accuracy for classification\n",
        "# problems. The metrics to collect are specified by name in an array. For example:\n",
        "\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "NfDQkRbCW-nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Fit Network\n",
        "\n",
        "Once the network is compiled, it can be fit, which means adapt the weights on a training dataset.\n",
        "\n",
        "Fitting the network requires the training data to be specified, both a matrix of input patterns, X,\n",
        "and an array of matching output patterns, y."
      ],
      "metadata": {
        "id": "3vP2ad0yZsPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of fitting a compiled model.**\n",
        "\n",
        "history = model.fit(X, y, batch_size=10, epochs=100)\n",
        "\n",
        "#\n",
        "\n",
        "#\n",
        "\n",
        "Once fit, a history object is returned that provides a summary of the performance of the\n",
        "model during training. \n",
        "\n",
        "This includes both the loss and any additional metrics specified when\n",
        "compiling the model, recorded each epoch."
      ],
      "metadata": {
        "id": "XbAN0vdta9_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can reduce the amount of information displayed to just the\n",
        "loss each epoch by setting the verbose argument to 2. \n",
        "\n",
        "You can turn off all output by setting\n",
        "verbose to 0. For example:\n",
        "\n",
        "#\n",
        "\n",
        "history = model.fit(X, y, batch_size=10, epochs=100, verbose=0)"
      ],
      "metadata": {
        "id": "4Lmrv82rbv6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 4. Evaluate Network**\n",
        "\n",
        "The model evaluates the loss across all of the test patterns, as well as any other metrics\n",
        "specified when the model was compiled, like classification accuracy. \n",
        "\n",
        "A list of evaluation metrics\n",
        "is returned. \n",
        "\n",
        "For example, for a model compiled with the accuracy metric, we could evaluate it\n",
        "on a new dataset as follows:\n",
        "\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "\n",
        "#\n",
        "\n",
        "#\n",
        "\n",
        "Turn off verbose:\n",
        "\n",
        "loss, accuracy = model.evaluate(X, y, verbose=0)"
      ],
      "metadata": {
        "id": "-H_Bc_aucGUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5. Make Predictions**\n",
        "\n",
        "Once we are satisfied with the performance of our fit model, we can use it to make predictions\n",
        "on new data. \n",
        "\n",
        "This is as easy as calling the *predict()* function on the model with an array of\n",
        "new input patterns. \n",
        "\n",
        "For example:\n",
        "\n",
        "predictions = model.predict(X)\n",
        "\n",
        "\n",
        "The predictions will be returned in the format provided by the output layer of the network.\n",
        "\n",
        "- In the case of a regression problem, these predictions may be in the format of the problem\n",
        "directly, provided by a linear activation function. \n",
        "\n",
        "- For a binary classification problem, the\n",
        "predictions may be an array of probabilities for the first class that can be converted to a 1 or 0\n",
        "by rounding.\n",
        "\n",
        "- For a multiclass classification problem, the results may be in the form of an array of\n",
        "probabilities (assuming a one hot encoded output variable) that may need to be converted to a\n",
        "single class output prediction using the *argmax()* NumPy function. \n",
        "\n",
        "Alternately, for classification\n",
        "problems, we can use the *predict_classes()* function that will automatically convert uncrisp\n",
        "predictions to crisp integer class values.\n",
        "\n",
        "predictions = model.predict_classes(X)"
      ],
      "metadata": {
        "id": "GHr7cr4ycwSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Network Models"
      ],
      "metadata": {
        "id": "uZIYp85nJiYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multilayer Perceptron\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "visible = Input(shape=(10,))\n",
        "hidden1 = Dense(10, activation='relu')(visible)\n",
        "hidden2 = Dense(20, activation='relu')(hidden1)\n",
        "hidden3 = Dense(10, activation='relu')(hidden2)\n",
        "output = Dense(1, activation='sigmoid')(hidden3)\n",
        "model = Model(inputs=visible, outputs=output)\n",
        "# summarize layers\n",
        "model.summary()\n",
        "# plot graph\n",
        "plot_model(model, to_file='multilayer_perceptron_graph.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "CmCYrdHSZOW1",
        "outputId": "585c3df6-6fbc-4016-a38b-a50fbdc95633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 10)]              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 20)                220       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                210       \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 551\n",
            "Trainable params: 551\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolutional Neural Network (CNN)\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "visible = Input(shape=(64,64,1))\n",
        "conv1 = Conv2D(32, (4,4), activation='relu')(visible)\n",
        "pool1 = MaxPooling2D()(conv1)\n",
        "conv2 = Conv2D(16, (4,4), activation='relu')(pool1)\n",
        "pool2 = MaxPooling2D()(conv2)\n",
        "hidden1 = Dense(10, activation='relu')(pool2)\n",
        "output = Dense(1, activation='sigmoid')(hidden1)\n",
        "model = Model(inputs=visible, outputs=output)\n",
        "# summarize layers\n",
        "model.summary()\n",
        "# plot graph\n",
        "plot_model(model, to_file='convolutional_neural_network.png')"
      ],
      "metadata": {
        "id": "cK1RW9X5a2jK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "315ec118-4058-4793-f4d9-3930971211a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 64, 64, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 61, 61, 32)        544       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 30, 30, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 27, 27, 16)        8208      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 13, 13, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 13, 13, 10)        170       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 13, 13, 1)         11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,933\n",
            "Trainable params: 8,933\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recurrent Neural Network (RNN)\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers.recurrent import LSTM\n",
        "visible = Input(shape=(100,1))\n",
        "hidden1 = LSTM(10)(visible)\n",
        "hidden2 = Dense(10, activation='relu')(hidden1)\n",
        "output = Dense(1, activation='sigmoid')(hidden2)\n",
        "model = Model(inputs=visible, outputs=output)\n",
        "# summarize layers\n",
        "model.summary()\n",
        "# plot graph\n",
        "plot_model(model, to_file='recurrent_neural_network.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "Tq4NYUk5GdHF",
        "outputId": "106dcd14-ce63-4cbf-ba9a-1ffd067bb81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 100, 1)]          0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 10)                480       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 601\n",
            "Trainable params: 601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III - Data Preparation\n",
        "### Data Preparation\n",
        "\n",
        "\n",
        "- How to Clean Text Manually and with NLTK\n",
        "\n",
        "- How to Prepare Text Data with scikit-learn\n",
        "\n",
        "- How to Prepare Text Data With Keras"
      ],
      "metadata": {
        "id": "WTFwEySUJ0bO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Clean Text Manually and with NLTK\n",
        "\n",
        "Text Cleaning Is Task Specific\n",
        "\n",
        "Take a moment to look at the text. What do you notice? Here's what I see:\n",
        "\n",
        "- It's plain text so there is no markup to parse (yay!).\n",
        "- The translation of the original German uses UK English (e.g. travelling).\n",
        "- The lines are artificially wrapped with new lines at about 70 characters (meh)\n",
        "- There are no obvious typos or spelling mistakes.\n",
        "- There's punctuation like commas, apostrophes, quotes, question marks, and more.\n",
        "- There's hyphenated descriptions like armour-like.\n",
        "- There's a lot of use of the em dash (-) to continue sentences (maybe replace with commas?).\n",
        "- There are names (e.g. Mr. Samsa)\n",
        "- There does not appear to be numbers that require handling (e.g. 1999)\n",
        "- There are section markers (e.g. II and III )."
      ],
      "metadata": {
        "id": "zyfz0axvKv_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual Tokenization\n",
        "\n",
        "# Split by Whitespace\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "tCn0mKqcG3v3",
        "outputId": "aecc994e-c3e2-4918-afb2-51c7eff09423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d14d0ace-2924-4b58-b7ec-88fb40432cf9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d14d0ace-2924-4b58-b7ec-88fb40432cf9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving metamorphosis_clean.txt to metamorphosis_clean.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load text\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "\n",
        "file.close()\n",
        "# split into words by white space\n",
        "words = text.split()\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRogIY7JQb29",
        "outputId": "14e8cc86-ab9f-46b8-fdcb-d8f4528b65c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffOne', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"What\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Words\n",
        "\n",
        "# Another approach might be to use the regex model (re) and split the document into words by\n",
        "# selecting for strings of alphanumeric characters (a-z, A-Z, 0-9 and ` '). For example:\n",
        "\n",
        "import re\n",
        "# load text\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split based on words only\n",
        "words = re.split(r'\\W+', text)\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwXYBkS1Qq0r",
        "outputId": "00f32048-3157-475e-984c-5c117252ac1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 's', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasn', 't', 'a', 'dream', 'His']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split by Whitespace and Remove Punctuation\n",
        "\n",
        "import string\n",
        "\n",
        "print(string.punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX-epeRfRSaJ",
        "outputId": "f4130989-eb7e-409d-9bf4-23665f8aab8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use regular expressions to select for the punctuation characters and use the sub()\n",
        "# function to replace them with nothing. For example:\n",
        "\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in words]"
      ],
      "metadata": {
        "id": "4j57V3DKR78s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can put all of this together, load the text file, split it into words by white space, then\n",
        "# translate each word to remove the punctuation.\n",
        "\n",
        "import string\n",
        "import re\n",
        "# load text\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words by white space\n",
        "words = text.split()\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in words]\n",
        "print(stripped[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUnXC1JpSXvw",
        "outputId": "5e48ad0d-e2d1-4249-f69e-1aa94192b780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffOne', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armourlike', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'Whats', 'happened', 'to', 'me', 'he', 'thought', 'It', 'wasnt', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sometimes text data may contain non-printable characters.\n",
        "\n",
        "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "result = [re_print.sub('', w) for w in words]\n",
        "print(result[:100])"
      ],
      "metadata": {
        "id": "qrfIX680SpkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "100a36fa-3256-4f17-987f-dcd9b662ff60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"What\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'It', \"wasn't\", 'a', 'dream.', 'His', 'room,', 'a', 'proper', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Case\n",
        "\n",
        "# We can convert all words to lowercase by calling the lower() function on each\n",
        "# word. For example:\n",
        "\n",
        "words = text.split()\n",
        "# convert to lower case\n",
        "words = [word.lower() for word in words]\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3Z12moyTXnl",
        "outputId": "440e1c73-5dd0-4526-ea1d-7a2d56a6882b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffone', 'morning,', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'he', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'his', 'many', 'legs,', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him,', 'waved', 'about', 'helplessly', 'as', 'he', 'looked.', '\"what\\'s', 'happened', 'to', 'me?\"', 'he', 'thought.', 'it', \"wasn't\", 'a', 'dream.', 'his', 'room,', 'a', 'proper', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization and Cleaning with NLTK"
      ],
      "metadata": {
        "id": "HKUU5uroUFw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download()"
      ],
      "metadata": {
        "id": "vTS5V_t9TsrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK script to split text into sentences.\n",
        "\n",
        "\n",
        "# nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "# load data\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8-vpR1c4J4-",
        "outputId": "5444fc06-1259-4b37-b35d-30ccca01e0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
            "himself transformed in his bed into a horrible vermin.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK script to split text into words.\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "# load data\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjEbdA5L4ma5",
        "outputId": "f42f8da1-9adb-4f79-dbf1-9ef4a27dadc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffOne', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', '.', 'His', 'many', 'legs', ',', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', '.', '``', 'What', \"'s\", 'happened', 'to']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK script to remove punctuation.\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "# load data\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "# remove all tokens that are not alphabetic\n",
        "words = [word for word in tokens if word.isalpha()]\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqMiK5_E5Xy7",
        "outputId": "d202c362-13c7-48a1-ba5a-d5ae414a7293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', 'waved', 'about', 'helplessly', 'as', 'he', 'looked', 'What', 'happened', 'to', 'me', 'he', 'thought', 'It', 'was', 'a', 'dream', 'His', 'room', 'a', 'proper', 'human', 'room', 'although']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out Stop Words (and Pipeline)\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)\n",
        "print(len(stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaSLE21r52FT",
        "outputId": "c090b50e-2439-4a9c-d8f4-886b8f406b6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop Words:\n",
        "\n",
        "You can see that they are all lower case and have punctuation removed. \n",
        "\n",
        "You could compare\n",
        "your tokens to the stop words and filter them out, but you must ensure that your text is prepared\n",
        "the same way. \n",
        "\n",
        "Let's demonstrate this with a small pipeline of text preparation including:\n",
        "-  Load the raw text.\n",
        "-  Split into tokens.\n",
        "-  Convert to lowercase.\n",
        "-  Remove punctuation from each token.\n",
        "-  Filter out remaining tokens that are not alphabetic.\n",
        "-  Filter out tokens that are stop words."
      ],
      "metadata": {
        "id": "FAh0M0A46npy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "# load data\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "# convert to lower case\n",
        "tokens = [w.lower() for w in tokens]\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "stripped = [re_punc.sub('', w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "words = [word for word in stripped if word.isalpha()]\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [w for w in words if not w in stop_words]\n",
        "print(words[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR5SqrQq6MCt",
        "outputId": "d83aaee0-646e-4509-a3d3-fef0893f1ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine', 'housed', 'nice', 'gilded', 'frame', 'showed', 'lady', 'fitted', 'fur', 'hat', 'fur', 'boa', 'sat', 'upright', 'raising', 'heavy', 'fur', 'muff', 'covered', 'whole', 'lower', 'arm', 'towards', 'viewer', 'gregor']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running this example, we can see that in addition to all of the other transforms, stop words\n",
        "like a and to have been removed. I note that we are still left with tokens like *nt*. The rabbit\n",
        "hole is deep; there's always more we can do."
      ],
      "metadata": {
        "id": "o0rU1YSu7jwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stem Words\n",
        "\n",
        "Stemming refers to the process of reducing each word to its root or base. For example *fishing,\n",
        "fished, fisher* all reduce to the stem **fish**.\n",
        "\n",
        "There are many stemming algorithms,\n",
        "although a popular and long-standing method is the *Porter Stemming* algorithm. This method\n",
        "is available in NLTK via the *PorterStemmer* class."
      ],
      "metadata": {
        "id": "LzsKlBLm7yRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NLTK script stem words.\n",
        "# the stemming implementation has also reduced\n",
        "# the tokens to lowercase, likely for internal look-ups in word tables.\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "# load data\n",
        "filename = 'metamorphosis_clean.txt'\n",
        "file = open(filename, 'rt')\n",
        "text = file.read()\n",
        "file.close()\n",
        "# split into words\n",
        "tokens = word_tokenize(text)\n",
        "# stemming of words\n",
        "porter = PorterStemmer()\n",
        "stemmed = [porter.stem(word) for word in tokens]\n",
        "print(stemmed[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JSZLLu37WlY",
        "outputId": "753cc95a-db44-4248-b993-561dde3b5fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\ufeffone', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', '.', 'He', 'lay', 'on', 'hi', 'armour-lik', 'back', ',', 'and', 'if', 'he', 'lift', 'hi', 'head', 'a', 'littl', 'he', 'could', 'see', 'hi', 'brown', 'belli', ',', 'slightli', 'dome', 'and', 'divid', 'by', 'arch', 'into', 'stiff', 'section', '.', 'the', 'bed', 'wa', 'hardli', 'abl', 'to', 'cover', 'it', 'and', 'seem', 'readi', 'to', 'slide', 'off', 'ani', 'moment', '.', 'hi', 'mani', 'leg', ',', 'piti', 'thin', 'compar', 'with', 'the', 'size', 'of', 'the', 'rest', 'of', 'him', ',', 'wave', 'about', 'helplessli', 'as', 'he', 'look', '.', '``', 'what', \"'s\", 'happen', 'to']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Text Cleaning Considerations\n",
        "\n",
        "Here is a shortlist of additional considerations when cleaning text:\n",
        "-  Handling large documents and large collections of text documents that do not fit into\n",
        "memory.\n",
        "-  Extracting text from markup like HTML, PDF, or other structured document formats.\n",
        "-  Transliteration of characters from other languages into English.\n",
        "-  Decoding Unicode characters into a normalized form, such as UTF8.\n",
        "-  Handling of domain specific words, phrases, and acronyms.\n",
        "-  Handling or removing numbers, such as dates and amounts.\n",
        "-  Locating and correcting common typos and misspellings."
      ],
      "metadata": {
        "id": "EzjocET19xxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Prepare Text Data with scikit-learn\n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "-  How to convert text to word count vectors with *CountVectorizer.*\n",
        "-  How to convert text to word frequency vectors with *TfidfVectorizer.*\n",
        "-  How to convert text to unique integers with *HashingVectorizer.*"
      ],
      "metadata": {
        "id": "pcNdxGTF-RTT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bag-of-Words Model\n",
        "\n",
        "The model is simple in that it throws away all of the order\n",
        "information in the words and focuses on the occurrence of words in a document.\n",
        "\n",
        "This can be done by assigning each word a unique number.\n",
        "\n",
        "Then any document we see can be encoded\n",
        "as a fixed-length vector with the length of the vocabulary of known words. \n",
        "\n",
        "The value in each position in the vector could be filled with a count or frequency of each word in the encoded\n",
        "document."
      ],
      "metadata": {
        "id": "ZWD7JyBh_ZWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scikit-learn library provides 3 different schemes that we can use, and we will briefly look at each.\n",
        "\n",
        "- Word Counts with **CountVectorizer**\n",
        "\n",
        "The CountVectorizer provides a simple way to both tokenize a collection of text documents\n",
        "and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n",
        "\n",
        "An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document. \n",
        "\n",
        "Because these vectors will\n",
        "contain a lot of zeros, we call them sparse. Python provides an efficient way of handling sparse vectors in the scipy.sparse package. \n",
        "\n",
        "The vectors returned from a call to transform() will\n",
        "be sparse vectors, and you can transform them back to NumPy arrays to look and better understand what is going on by calling the toarray() function. \n",
        "\n",
        "Below is an example of using\n",
        "the CountVectorizer to tokenize, build a vocabulary, and then encode a document.\n"
      ],
      "metadata": {
        "id": "a2XGMqBMARtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# summarize encoded vector\n",
        "print('\\nshape of the encoded document: ',vector.shape)\n",
        "print(type(vector))\n",
        "print()\n",
        "print('sparse vector:\\n',vector)\n",
        "print()\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym9qzGiI8kC0",
        "outputId": "3c0f936e-04f3-4286-b364-794b1ae34d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "\n",
            "shape of the encoded document:  (1, 8)\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "\n",
            "sparse vector:\n",
            "   (0, 0)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 7)\t2\n",
            "\n",
            "[[1 1 1 1 1 1 1 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importantly, the same vectorizer can be used on documents that contain words not included\n",
        "# in the vocabulary.\n",
        "\n",
        "\n",
        "# encode another document\n",
        "text2 = [\"the puppy\"]\n",
        "vector = vectorizer.transform(text2)\n",
        "print(vector.toarray())\n",
        "\n",
        "\n",
        "# The encoded vectors can then be used directly with a machine learning algorithm."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxfzawNnBQyT",
        "outputId": "d2a4f1c1-c976-4e71-c98d-b770cb47a052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Word Frequencies with TfidfVectorizer**\n",
        "\n",
        "Without going into the math, TF-IDF are word frequency scores that try to highlight\n",
        "words that are more interesting, e.g. frequent in a document but not across documents.\n",
        "\n",
        "- Term Frequency (tf): This summarizes how often a given word appears within a document.\n",
        "- Inverse Document Frequency (idf): This downscales words that appear a lot across documents.\n",
        "\n",
        "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents.\n",
        "\n",
        "Alternately, if you already have a\n",
        "learned *CountVectorizer*, you can use it with a **TfidfTransformer** to just calculate the inverse\n",
        "document frequencies and start encoding documents."
      ],
      "metadata": {
        "id": "j9ut0sxJDX5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
        "\"The dog.\",\n",
        "\"The fox\"]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "print('\\ntf_idf vector:\\n',vectorizer.idf_)\n",
        "\n",
        "# encode document, the first text\n",
        "vector = vectorizer.transform([text[0]])\n",
        "\n",
        "# summarize encoded vector\n",
        "print('\\nvector shape: ', vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV6w2rloC8dH",
        "outputId": "2cf1646d-93a4-4a1c-a3e1-d9049db4c936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "\n",
            "tf_idf vector:\n",
            " [1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.        ]\n",
            "\n",
            "vector shape:  (1, 8)\n",
            "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
            "  0.36388646 0.42983441]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hashing with HashingVectorizer\n",
        "\n",
        "Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large. This, in turn, will require large vectors for encoding\n",
        "documents and impose large requirements on memory and slow down algorithms. \n",
        "\n",
        "A clever work\n",
        "around is to use a one way hash of words to convert them to integers. \n",
        "\n",
        "The clever part is that\n",
        "no vocabulary is required and you can choose an arbitrary-long fixed length vector.\n",
        "\n",
        "A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word\n",
        "(which may not matter for many supervised learning tasks).\n",
        "\n",
        "\n",
        "The values\n",
        "of the encoded document correspond to normalized word counts by default in the range of -1 to 1, but could be made simple integer counts by changing the default configuration."
      ],
      "metadata": {
        "id": "B6KYMM72GQ51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = HashingVectorizer(n_features=20)\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd-Dcft4EyBQ",
        "outputId": "15417549-dafc-4b27-d993-2b5113b06bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 20)\n",
            "[[ 0.          0.          0.          0.          0.          0.33333333\n",
            "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
            "   0.          0.          0.         -0.33333333  0.          0.\n",
            "  -0.66666667  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Prepare Text Data With Keras"
      ],
      "metadata": {
        "id": "V9qSyWeiKEYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras deep learning library provides some basic tools to help you\n",
        "prepare your text data. \n",
        "\n",
        "In this tutorial, you will discover how you can use Keras to prepare\n",
        "your text data.\n",
        "\n",
        "- About the convenience methods that you can use to quickly prepare text data.\n",
        "- The Tokenizer API that can be fit on training data and used to encode training, validation,\n",
        "and test documents.\n",
        "- The range of 4 different document encoding schemes offered by the Tokenizer API.\n",
        "\n",
        "**Split Words with text to word sequence**\n",
        "\n",
        "Keras provides the\n",
        "text to word sequence() function that you can use to split text into a list of words. \n",
        "\n",
        "By default, this function automatically does 3 things:\n",
        "\n",
        "- Splits words by space.\n",
        "- Filters out punctuation.\n",
        "- Converts text to lowercase (lower=True).\n",
        "\n",
        "You can change any of these defaults by passing arguments to the function.\n",
        "\n"
      ],
      "metadata": {
        "id": "0vnO0pcaK2RU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# tokenize the document\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "XxeK9CFNH-M7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c68e320-29ec-4133-bed0-558de0a6d6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoding with one hot**\n",
        "\n",
        "In addition to the text, the vocabulary size (total words) must be specified. \n",
        "\n",
        "This could be the\n",
        "total number of words in the document or more if you intend to encode additional documents\n",
        "that contains additional words.\n",
        "\n",
        "We can use the text to word sequence() function from the previous section to split the\n",
        "document into words and then use a set to represent only the unique words in the document."
      ],
      "metadata": {
        "id": "0UBtcPlkLp8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: Given the stochastic nature of neural networks, your specific results may vary. Consider\n",
        "# running the example a few times.\n",
        "\n",
        "# Example of one hot encoding.\n",
        "\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "# integer encode the document\n",
        "result = one_hot(text, round(vocab_size*1.3))\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNlDUlhnLmNP",
        "outputId": "7d248227-7fbc-4a82-c47a-5846b603ce2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "[7, 7, 8, 5, 8, 7, 7, 8, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hash Encoding with hashing trick**\n",
        "\n",
        "A limitation of integer and count base encodings is that they must maintain a vocabulary of\n",
        "words and their mapping to integers. \n",
        "\n",
        "An alternative to this approach is to use a one-way hash\n",
        "function to convert words to integers. \n",
        "\n",
        "This avoids the need to keep track of a vocabulary, which\n",
        "is faster and requires less memory.\n",
        "\n"
      ],
      "metadata": {
        "id": "_Rkf6zoIMUoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of hash encoding.\n",
        "\n",
        "from keras.preprocessing.text import hashing_trick\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "# integer encode the document\n",
        "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmrvXJc2MSgR",
        "outputId": "578ee5bc-fc8d-41e1-fa14-56f406e3c899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer API**\n",
        "\n",
        "\n",
        "Keras provides a more sophisticated API for preparing text that can be fit and reused to prepare multiple text documents.\n",
        "\n",
        "This may be the preferred approach for large projects. \n",
        "\n",
        "Keras provides\n",
        "the Tokenizer class for preparing text documents for deep learning. \n",
        "\n",
        "The Tokenizer must be constructed and then fit on either raw text documents or integer encoded text documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "-uQqZf8eNKSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "# define 5 documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)"
      ],
      "metadata": {
        "id": "YAuDsaXANizp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once fit, the Tokenizer provides 4 attributes that you can use to query what has been\n",
        "learned about your documents:\n",
        "- word **counts**: A dictionary mapping of words and their occurrence counts when the Tokenizer was fit.\n",
        "- word **docs**: A dictionary mapping of words and the number of documents that each appears in.\n",
        "- word **index**: A dictionary of words and their uniquely assigned integers.\n",
        "- **document count**: A dictionary mapping and the number of documents they appear in calculated during the fit."
      ],
      "metadata": {
        "id": "Xt_0kvK9NngZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize what was learned\n",
        "print('word_counts:\\n', t.word_counts)\n",
        "print('\\ndocument_count:\\n',t.document_count)\n",
        "print('\\n\\nword_index:\\n',t.word_index)\n",
        "print('\\nword_docs:\\n',t.word_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdGdo0_SNGiJ",
        "outputId": "e61d4817-726b-4214-c94f-96f99c98d707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_counts:\n",
            " OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
            "\n",
            "document_count:\n",
            " 5\n",
            "\n",
            "\n",
            "word_index:\n",
            " {'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
            "\n",
            "word_docs:\n",
            " defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'work': 2, 'good': 1, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the Tokenizer has been fit on training data, it can be used to encode documents in\n",
        "the train or test datasets. \n",
        "\n",
        "The *texts_to_matrix()* function on the *Tokenizer* can be used to\n",
        "create one vector per document provided per input. \n",
        "\n",
        "The length of the vectors is the total size\n",
        "of the vocabulary. \n",
        "\n",
        "This function provides a suite of standard bag-of-words model text encoding\n",
        "schemes that can be provided via a mode argument to the function.\n",
        "\n",
        "The modes available include:\n",
        "- **binary**: Whether or not each word is present in the document. This is the **default**.\n",
        "- **count**: The count of each word in the document.\n",
        "- **tfidf**: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
        "- **freq**: The frequency of each word as a ratio of words within each document.\n",
        "\n",
        "**The Tokenizer will be the key way we will prepare text for word embeddings throughout this book.**"
      ],
      "metadata": {
        "id": "z6Q6sMI7OUUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can put all of this together with a worked example.\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# define 5 documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)\n",
        "# summarize what was learned\n",
        "print('word_counts:\\n', t.word_counts)\n",
        "print('\\ndocument_count:\\n',t.document_count)\n",
        "print('\\n\\nword_index:\\n',t.word_index)\n",
        "print('\\nword_docs:\\n',t.word_docs)\n",
        "\n",
        "# integer encode documents\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
        "print('\\n\\ntexts_to_matrix:\\n',encoded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXvkc0foOL7g",
        "outputId": "89f1cdd9-993e-439a-9a4e-1e05def87924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_counts:\n",
            " OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
            "\n",
            "document_count:\n",
            " 5\n",
            "\n",
            "\n",
            "word_index:\n",
            " {'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
            "\n",
            "word_docs:\n",
            " defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'work': 2, 'good': 1, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n",
            "\n",
            "\n",
            "texts_to_matrix:\n",
            " [[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part IV Bag-of-Words"
      ],
      "metadata": {
        "id": "hAX2Qg7UKonM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bag-of-words model is a way of representing text data when modeling text with machine learning algorithms.\n",
        "\n",
        "In this\n",
        "tutorial, you will discover the bag-of-words model for feature extraction in natural language\n",
        "processing. \n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "- What the bag-of-words model is and why it is needed to represent text.\n",
        "- How to develop a bag-of-words model for a collection of documents.\n",
        "- How to use different techniques to prepare a vocabulary and score words.\n",
        "\n",
        "Machine learning algorithms cannot work\n",
        "with raw text directly; the text must be converted into numbers. \n",
        "\n",
        "Specifically, vectors of numbers.\n",
        "\n",
        "*In language processing, the vectors x are derived from textual data, in order to reflect various linguistic properties of the text.*\n",
        "\n",
        "This is called feature extraction or feature encoding. \n",
        "\n",
        "A popular and simple method of feature extraction with text data is called the bag-of-words model of text.\n",
        "\n",
        "The approach is very simple and \n",
        "flexible, and can be used in a myriad of ways for extracting features from documents. \n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document. \n",
        "\n",
        "It involves two things:\n",
        "- A vocabulary of known words.\n",
        "- A measure of the presence of known words.\n",
        "\n",
        "It is called a bag-of-words , because any information about the order or structure of words in the document is discarded. \n",
        "\n",
        "The model is only concerned with whether known words **occur** in\n",
        "the document, **not where** in the document."
      ],
      "metadata": {
        "id": "WobS0Tw6QhdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*A very common feature extraction procedures for sentences and documents is the bag-of-words approach (BOW). In this approach, we look at the histogram of the\n",
        "words within the text, i.e. considering each word count as a feature.*\n",
        "\n",
        "The bag-of-words can be as simple or complex as you like. \n",
        "\n",
        "The complexity comes both in deciding how to design\n",
        "the vocabulary of known words (or tokens) and how to score the presence of known words.\n",
        "\n"
      ],
      "metadata": {
        "id": "SI_1NKvQT9YJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of the Bag-of-Words Model**\n",
        "\n",
        "Sample of text from A Tale of Two Cities by Charles Dickens.\n",
        "\n",
        "It was the best of times,\n",
        "\n",
        "it was the worst of times,\n",
        "\n",
        "it was the age of wisdom,\n",
        "\n",
        "it was the age of foolishness,\n",
        "\n",
        "For this small example, let's treat each line as a separate document and the 4 lines as our entire corpus of documents.\n",
        "\n",
        "**list of all of the words in our model vocabulary.**\n",
        "- it\n",
        "- was\n",
        "- the\n",
        "- best\n",
        "- of\n",
        "- times\n",
        "- worst\n",
        "- age\n",
        "- wisdom\n",
        "- foolishness\n",
        "\n",
        "That is a vocabulary of 10 words from a corpus containing 24 words."
      ],
      "metadata": {
        "id": "SSxH4EhnUsI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Document Vectors**\n",
        "\n",
        "Because\n",
        "we know the vocabulary has 10 words, we can use a fixed-length document representation of 10, with one position in the vector to score each word. \n",
        "\n",
        "The simplest scoring method is to mark the\n",
        "presence of words as a boolean value, 0 for absent, 1 for present.\n",
        "\n",
        "first document (It was the best of times) and convert it into a binary vector. The scoring of the document would look as follows:\n",
        "\n",
        "- it = 1\n",
        "-was = 1\n",
        "-the = 1\n",
        "-best = 1\n",
        "-of = 1\n",
        "-times = 1\n",
        "-worst = 0\n",
        "-age = 0\n",
        "-wisdom = 0\n",
        "-foolishness = 0\n",
        "\n",
        "As a binary vector, this would look as follows:\n",
        "\n",
        "[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
        "\n",
        "The other three documents would look as follows:\n",
        "\n",
        "- \"it was the worst of times\" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
        "- \"it was the age of wisdom\" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
        "- \"it was the age of foolishness\" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n",
        "\n",
        "New documents that\n",
        "overlap with the vocabulary of known words, but may contain words outside of the vocabulary, can still be encoded, where only the **occurrence of known words** are scored and unknown words are ignored."
      ],
      "metadata": {
        "id": "XkyOpoISVbXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Managing Vocabulary**\n",
        "\n",
        "You can\n",
        "imagine that for a very large corpus, such as thousands of books, that the length of the vector\n",
        "might be thousands or millions of positions. Further, each document may contain very few of the known words in the vocabulary.\n",
        "\n",
        "This results in a vector with lots of zero scores, called a sparse vector or sparse representation.\n",
        "\n",
        "Sparse vectors require more memory and computational resources when modeling and the vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms. \n",
        "\n",
        "As such, there is pressure to decrease the size of the vocabulary when\n",
        "using a bag-of-words model.\n",
        "\n",
        "There are simple text cleaning techniques that can be used as a first step, such as:\n",
        "\n",
        "- Ignoring case.\n",
        "- Ignoring punctuation.\n",
        "- Ignoring frequent words that don't contain much information, called **stop words**, like a, of, etc.\n",
        "- Fixing misspelled words.\n",
        "- Reducing words to their stem (e.g. play from playing) using stemming algorithms.\n",
        "\n",
        "-----------------------------------------\n",
        "A more sophisticated approach is to create a vocabulary of grouped words. This both\n",
        "changes the scope of the vocabulary and allows the bag-of-words to capture a little bit more\n",
        "meaning from the document. \n",
        "\n",
        "In this approach, each word or token is called a *gram*. \n",
        "\n",
        "Creating a vocabulary of *two-word pairs* is, in turn, called a *bigram* model. \n",
        "\n",
        "Again, only the bigrams that\n",
        "appear in the corpus are modeled, not all possible bigrams.\n",
        "\n",
        "An n-gram is an n-token sequence of words: a 2-gram (more commonly called a\n",
        "bigram) is a two-word sequence of words like \\please turn\", \\turn your\", or \\your homework\", and a 3-gram (more commonly called a **trigram**) is a three-word sequence of words like \\please turn your\", or \\turn your homework\".\n",
        "- \n",
        "Often a simple\n",
        "bigram approach is better than a 1-gram bag-of-words model for tasks like documentation\n",
        "classification."
      ],
      "metadata": {
        "id": "xw6NIj5NXxUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scoring Words**\n",
        "\n",
        "\n",
        "- Word Hashing: \n",
        "\n",
        "    Words are hashed deterministically to the same integer index in the target hash space. A binary score or count can then be used to score the word. \n",
        "\n",
        "    This is called the hash trick or feature\n",
        "    hashing. \n",
        "\n",
        "    The challenge is to choose a hash space to accommodate the chosen vocabulary size to minimize the probability of collisions and trade-of sparsity.\n",
        "\n",
        "- TF-IDF\n",
        "    \n",
        "    **Term Frequency (TF)**: is a scoring of the frequency of the word in the current document.\n",
        "    \n",
        "    **Inverse Document Frequency (IDF)**: is a scoring of how rare the word is across documents.\n",
        "    \n",
        "    The scores are a weighting where not all words are equally as important or interesting. \n",
        "    \n",
        "    The scores have the effect of highlighting words that are distinct (contain useful information) in a given document."
      ],
      "metadata": {
        "id": "G_86OSeCaRFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limitations of Bag-of-Words**\n",
        "\n",
        "The bag-of-words model is very simple to understand and implement and offers a lot of exibility for customization on your specific text data. \n",
        "\n",
        "It has been used with great success on prediction\n",
        "problems like language modeling and documentation classification. \n",
        "\n",
        "Nevertheless, it suffers from some shortcomings, such as:\n",
        "\n",
        "- **Vocabulary**: The vocabulary requires careful design, most specifically in order to manage\n",
        "the size, which impacts the sparsity of the document representations.\n",
        "\n",
        "- **Sparsity**: Sparse representations are harder to model both for computational reasons\n",
        "(space and time complexity) and also for information reasons, where the challenge is for\n",
        "the models to harness so little information in such a large representational space.\n",
        "\n",
        "- **Meaning**: Discarding word order ignores the context, and in turn meaning of words in\n",
        "the document (semantics). Context and meaning can offer a lot to the model, that if\n",
        "modeled could tell the difference between the same words differently arranged (this is interesting vs is this interesting), synonyms (old bike vs used bike), and much more."
      ],
      "metadata": {
        "id": "6Cx4f8gEcQ07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How to Prepare Movie Review Data for Sentiment Analysis**\n",
        "\n",
        "**Text data preparation is different for each problem.**\n",
        "\n",
        "Preparation starts with simple steps, like\n",
        "loading data, but quickly gets diffcult with cleaning tasks that are very specific to the data you\n",
        "are working with. \n",
        "\n",
        "You need help as to where to begin and what order to work through the steps\n",
        "from raw data to data ready for modeling. \n",
        "\n",
        "In this tutorial, you will discover how to prepare\n",
        "movie review text data for sentiment analysis, step-by-step. \n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "- How to load text data and clean it to remove punctuation and other non-words.\n",
        "- How to develop a vocabulary, tailor it, and save it to file.\n",
        "- How to prepare movie reviews using cleaning and a pre-defined vocabulary and save them to new files ready for modeling.\n",
        "\n",
        "This tutorial is divided into the following parts:\n",
        "1. Movie Review Dataset\n",
        "2. Load Text Data\n",
        "3. Clean Text Data\n",
        "4. Develop Vocabulary\n",
        "5. Save Prepared Data\n",
        "\n",
        "Our data contains 1000 positive and 1000 negative reviews all written before 2002,\n",
        "with a cap of 20 reviews per author (312 authors total) per category. We refer to this corpus as the polarity dataset.\n",
        "\n",
        "The data has been cleaned up somewhat, for example:\n",
        "- The dataset is comprised of only English reviews.\n",
        "- All text has been converted to lowercase.\n",
        "- There is white space around punctuation like periods, commas, and brackets.\n",
        "- Text has been split into one sentence per line."
      ],
      "metadata": {
        "id": "MdnyjSomr73n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Because there are lots of files to be read in memory, I have uploaded the folders in \n",
        "# Google Drive, so I have to mount it first, then read the data\n",
        "\n",
        "# In this method the Authentication of google drive will open a new window.\n",
        "# There are also other ways to inform the Auth to google in program text\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')     # Mounting Google Drive in Colab"
      ],
      "metadata": {
        "id": "gITI5xVPQCoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fabab835-de25-4950-8e1f-929e0e52db60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the reading of the file from Drive\n",
        "\n",
        "with open('/content/gdrive/My Drive/txt_sentoken/neg/cv000_29416.txt', 'r') as f:\n",
        "    text_neg = f.read()\n",
        "# print(text_neg)"
      ],
      "metadata": {
        "id": "cCtd6AttKrdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can turn this into a function called load doc() that takes a filename of the document to load and\n",
        "# returns the text.\n",
        "\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    with open(filename, 'r') as f:\n",
        "        text = f.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "metadata": {
        "id": "_ZHgnDRuBWmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%capture   ------------------------->>>   to hide the cell output\n",
        "\n",
        "%%capture\n",
        "\n",
        "# We have two directories each with 1,000 documents each. We can process each directory in\n",
        "# turn by first getting a list of files in the directory using the listdir() function, then loading\n",
        "# each file in turn.\n",
        "\n",
        "\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    with open(filename, 'r') as f:\n",
        "        text = f.read()\n",
        "    # close the file\n",
        "    return text\n",
        "\n",
        "# specify directory in Google Drive to load in Colab\n",
        "directory = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "\n",
        "from os import listdir\n",
        "\n",
        "# walk through all files in the folder\n",
        "i= 0 \n",
        "for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "    if not filename.endswith(\".txt\"):\n",
        "        next\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load document\n",
        "    doc = load_doc(path)\n",
        "    i += 1\n",
        "    print(i, 'Loaded %s' % filename)\n"
      ],
      "metadata": {
        "id": "DMQiJcbwB8Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOJIvntYIaq2",
        "outputId": "e8d90f99-2d85-4160-c847-2c1a042c39ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Below is an updated version of cleaning this review for one file cv000_29416.txt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "\n",
        "# # load doc into memory\n",
        "# def load_doc(filename):\n",
        "    # # open the file as read only\n",
        "    # file = open(filename, 'r')\n",
        "    # # read all text\n",
        "    # text = file.read()\n",
        "    # # close the file\n",
        "    # file.close()\n",
        "    # return text\n",
        "\n",
        "directory = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "filename = 'cv000_29416.txt'\n",
        "path = directory + '/' + filename\n",
        "\n",
        "# load the document\n",
        "text = load_doc(path)\n",
        "\n",
        "# split into tokens by white space\n",
        "tokens = text.split()\n",
        "\n",
        "# prepare regex for char filtering\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "\n",
        "# remove punctuation from each word\n",
        "tokens = [re_punc.sub('', w) for w in tokens]\n",
        "\n",
        "# remove remaining tokens that are not alphabetic\n",
        "tokens = [word for word in tokens if word.isalpha()]\n",
        "\n",
        "# filter out stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [w for w in tokens if not w in stop_words]\n",
        "\n",
        "# filter out short tokens\n",
        "tokens = [word for word in tokens if len(word) > 1]\n",
        "print(tokens)\n",
        "print('\\nNumber of Tokens:',len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijkGco4uFMX2",
        "outputId": "7e9bf28b-3b6d-48be-ab17-3463662c66f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'whats', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mindfuck', 'movie', 'teen', 'generation', 'touches', 'cool', 'idea', 'presents', 'bad', 'package', 'makes', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'films', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'ways', 'making', 'types', 'films', 'folks', 'didnt', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problems', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'starts', 'normal', 'downshifts', 'fantasy', 'world', 'audience', 'member', 'idea', 'whats', 'going', 'dreams', 'characters', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparitions', 'disappearances', 'looooot', 'chase', 'scenes', 'tons', 'weird', 'things', 'happen', 'simply', 'explained', 'personally', 'dont', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'films', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minutes', 'make', 'things', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flicks', 'like', 'actually', 'figured', 'halfway', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'didnt', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movies', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'visions', 'minutes', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'dont', 'know', 'really', 'need', 'see', 'giving', 'us', 'different', 'scenes', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'shows', 'mightve', 'pretty', 'decent', 'teen', 'mindfuck', 'movie', 'somewhere', 'guess', 'suits', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actors', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'holds', 'throughout', 'entire', 'film', 'actually', 'feeling', 'characters', 'unraveling', 'overall', 'film', 'doesnt', 'stick', 'doesnt', 'entertain', 'confusing', 'rarely', 'excites', 'feels', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kids', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'sitting', 'shelves', 'ever', 'since', 'whatever', 'skip', 'wheres', 'joblo', 'coming', 'nightmare', 'elm', 'street', 'blair', 'witch', 'crow', 'crow', 'salvation', 'lost', 'highway', 'memento', 'others', 'stir', 'echoes']\n",
            "\n",
            "Number of Tokens: 330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can put this into a function called clean doc() and test it on another review, this time\n",
        "# a positive review.\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "29-_0MooFMVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the document and test the function\n",
        "\n",
        "directory = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "filename = 'cv000_29416.txt'\n",
        "path = directory + '/' + filename\n",
        "\n",
        "text = load_doc(path)\n",
        "tokens = clean_doc(text)\n",
        "print(tokens)\n",
        "print('\\nNumber of Tokens:',len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YdoqDHuFMSu",
        "outputId": "f160e3fb-22b4-44d0-f020-4499b09bd0b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'whats', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'mindfuck', 'movie', 'teen', 'generation', 'touches', 'cool', 'idea', 'presents', 'bad', 'package', 'makes', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'films', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'ways', 'making', 'types', 'films', 'folks', 'didnt', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problems', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'starts', 'normal', 'downshifts', 'fantasy', 'world', 'audience', 'member', 'idea', 'whats', 'going', 'dreams', 'characters', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparitions', 'disappearances', 'looooot', 'chase', 'scenes', 'tons', 'weird', 'things', 'happen', 'simply', 'explained', 'personally', 'dont', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'films', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minutes', 'make', 'things', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flicks', 'like', 'actually', 'figured', 'halfway', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'didnt', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movies', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'visions', 'minutes', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'dont', 'know', 'really', 'need', 'see', 'giving', 'us', 'different', 'scenes', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'shows', 'mightve', 'pretty', 'decent', 'teen', 'mindfuck', 'movie', 'somewhere', 'guess', 'suits', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actors', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'holds', 'throughout', 'entire', 'film', 'actually', 'feeling', 'characters', 'unraveling', 'overall', 'film', 'doesnt', 'stick', 'doesnt', 'entertain', 'confusing', 'rarely', 'excites', 'feels', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kids', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'sitting', 'shelves', 'ever', 'since', 'whatever', 'skip', 'wheres', 'joblo', 'coming', 'nightmare', 'elm', 'street', 'blair', 'witch', 'crow', 'crow', 'salvation', 'lost', 'highway', 'memento', 'others', 'stir', 'echoes']\n",
            "\n",
            "Number of Tokens: 330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Develop Vocabulary**\n",
        "\n",
        "A part of preparing text for sentiment analysis involves defining and\n",
        "tailoring the vocabulary of words supported by the model. \n",
        "\n",
        "We can do this by loading all of the\n",
        "documents in the dataset and building a set of words. We may decide to support all of these words, or perhaps discard some. \n",
        "\n",
        "The final chosen vocabulary can then be saved to file for later\n",
        "use, such as filtering words in new documents in the future.\n",
        "\n",
        "We can keep track of the vocabulary in a Counter, which is a dictionary of words and their count with some additional convenience functions. \n",
        "\n",
        "We need to develop a new function to process a document and add it to the vocabulary. \n",
        "\n",
        "The function needs to load a document by calling the\n",
        "previously developed *load_doc()* function. \n",
        "\n",
        "It needs to clean the loaded document using the\n",
        "previously developed *clean_doc()* function, then it needs to add all the tokens to the Counter, and update counts. \n",
        "\n",
        "We can do this last step by calling the update() function on the counter\n",
        "object. \n",
        "\n",
        "Below is a function called add doc to vocab() that takes as arguments a document\n",
        "filename and a Counter vocabulary."
      ],
      "metadata": {
        "id": "LTDfaVTMMSgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc and add to vocab\n",
        "\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "    # load doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # update counts\n",
        "    vocab.update(tokens)"
      ],
      "metadata": {
        "id": "jWKwsnRgFMKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we can use our template above for processing all documents in a directory called\n",
        "# process docs() and update it to call add doc to vocab().\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            next\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # add doc to vocab\n",
        "        add_doc_to_vocab(path, vocab)"
      ],
      "metadata": {
        "id": "3rdl2B1YNkAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    with open(filename, 'r') as f:\n",
        "        text = f.read()\n",
        "    # close the file\n",
        "    return text\n",
        "\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "    # load doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # update counts\n",
        "    vocab.update(tokens)\n",
        "\n",
        "\n",
        "def process_docs(directory, vocab):\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            next\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # add doc to vocab\n",
        "        add_doc_to_vocab(path, vocab)\n",
        "\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "\n",
        "process_docs(directory_neg, vocab)\n",
        "len_neg = len(vocab)\n",
        "print('length Vocab Negative', len_neg)\n",
        "\n",
        "process_docs(directory_pos, vocab)\n",
        "print('length Vocab Positive', len(vocab) - len_neg)\n",
        "\n",
        "# print the size of the vocab\n",
        "print('\\n\\nTotal Size:',len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu1loAOsNj9_",
        "outputId": "9ca41db9-3a06-42b4-e613-069825c268f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length Vocab Negative 32010\n",
            "length Vocab Positive 14547\n",
            "\n",
            "\n",
            "Total Size: 46557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7nbuq4CNj7s",
        "outputId": "5906d049-3952-49bd-f78b-3ab879eb46e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perhaps the least common words, those that only appear once across all reviews, are notpredictive. \n",
        "\n",
        "Perhaps some of the most common words are not useful too. These are good\n",
        "questions and really should be tested with a specific predictive model. \n",
        "\n",
        "Generally, words that only appear once or a few times across 2,000 reviews are probably not predictive and can be removed from the vocabulary, greatly cutting down on the tokens we need to model. \n",
        "\n",
        "We can do this by stepping through words and their counts and only keeping those with a count above a chosen threshold. Here we will use 5 occurrences."
      ],
      "metadata": {
        "id": "pkkROEOcRgRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keep tokens with > 5 occurrence\n",
        "\n",
        "\n",
        "# This reduces the vocabulary from 46,557 to 14,803 words\n",
        "\n",
        "\n",
        "min_occurrence = 5\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb_oW6vNNj5F",
        "outputId": "8918511a-9542-47cb-8101-d238625527a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to save the vocabulary as ASCII with one\n",
        "# word per line. Below defines a function called save_list() to save a list of items, in this case,\n",
        "# tokens to file, one per line.\n",
        "\n",
        "\n",
        "def save_list(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "_PBzU8FzNj2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    with open(filename, 'r') as f:\n",
        "        text = f.read()\n",
        "    # close the file\n",
        "    return text\n",
        "\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "    # load doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # update counts\n",
        "    vocab.update(tokens)\n",
        "\n",
        "\n",
        "def process_docs(directory, vocab):\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "    # skip files that do not have the right extension\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            next\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # add doc to vocab\n",
        "        add_doc_to_vocab(path, vocab)\n",
        "\n",
        "\n",
        "def save_list(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "\n",
        "process_docs(directory_neg, vocab)\n",
        "len_neg = len(vocab)\n",
        "print('length Vocab Negative', len_neg)\n",
        "\n",
        "process_docs(directory_pos, vocab)\n",
        "print('length Vocab Positive', len(vocab) - len_neg)\n",
        "\n",
        "# print the size of the vocab\n",
        "print('\\n\\nTotal Size:',len(vocab))\n",
        "\n",
        "min_occurrence = 5\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
        "print('\\nNumber of Tokens after removing low frequency words:\\n',len(tokens))\n",
        "\n",
        "save_list(tokens, '/content/gdrive/My Drive/txt_sentoken/vocab.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wq2iHp7iR1hH",
        "outputId": "bb5f1c1d-10dc-4192-b0f6-a8431dc0ead4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length Vocab Negative 32010\n",
            "length Vocab Positive 14547\n",
            "\n",
            "\n",
            "Total Size: 46557\n",
            "\n",
            "Number of Tokens after removing low frequency words:\n",
            " 14803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save Prepared Data**\n",
        "\n",
        "We can use the data cleaning and chosen vocabulary to prepare each movie review and save the prepared versions of the reviews ready for modeling.\n",
        "\n",
        "This is a good practice as it decouples\n",
        "the data preparation from modeling, allowing you to focus on modeling and circle back to data prep if you have new ideas. \n",
        "\n",
        "We can start off by loading the vocabulary from *vocab.txt*.\n",
        "\n"
      ],
      "metadata": {
        "id": "umGEf2V2TxoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the saved vocabulary\n",
        "vocab_filename = '/content/gdrive/My Drive/txt_sentoken/vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4-F1CVtR1eU",
        "outputId": "f6d63134-2a00-4567-8106-e7ae98d5b7dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can clean the reviews, use the loaded vocab to filter out unwanted tokens, and save the clean reviews in a new file. \n",
        "\n",
        "One approach could be to save all the positive reviews\n",
        "in one file and all the negative reviews in another file, with the filtered tokens separated by white space for each review on separate lines. \n",
        "\n",
        "First, we can define a function to process a document, clean it, filter it, and return it as a single line that could be saved in a file. \n",
        "\n",
        "Below defines the *doc_to_line()* function to do just that, taking a filename and vocabulary (as a set) as arguments. \n",
        "\n",
        "It calls the previously defined *load_doc()* function to load the document and\n",
        "*clean_doc()* to tokenize the document."
      ],
      "metadata": {
        "id": "mWlehIBxUxBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to filter a review by the vocabulary\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "    # load the doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # filter by vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "JY4HW6q0R1b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can define a new version of *process_docs()* to step through all reviews in a folder\n",
        "and convert them to lines by calling *doc_to_line()* for each document. \n",
        "\n",
        "A list of lines is then returned.\n",
        "\n",
        "We can then call process docs() for both the directories of positive and negative reviews,\n",
        "then call save list() from the previous section to save each list of processed reviews to a file.\n",
        "\n",
        "Running the example saves two new files, negative.txt and positive.txt, that contain the prepared negative and positive reviews respectively. \n",
        "\n",
        "The data is ready for use in a bag-of-words or even word embedding model."
      ],
      "metadata": {
        "id": "fb13Ndipo3eQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "lines = list()\n",
        "# walk through all files in the folder\n",
        "for filename in listdir(directory):\n",
        "# skip files that do not have the right extension\n",
        "if not filename.endswith(\".txt\"):\n",
        "next\n",
        "# create the full path of the file to open\n",
        "path = directory + '/' + filename\n",
        "# load and clean the doc\n",
        "line = doc_to_line(path, vocab)\n",
        "# add to list\n",
        "lines.append(line)\n",
        "return lines"
      ],
      "metadata": {
        "id": "iKXW64wQR1Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "    # load the doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # filter by vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    return ' '.join(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "    lines = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "        # skip files that do not have the right extension\n",
        "        if not filename.endswith(\".txt\"):\n",
        "            next\n",
        "    # create the full path of the file to open\n",
        "    path = directory + '/' + filename\n",
        "    # load and clean the doc\n",
        "    line = doc_to_line(path, vocab)\n",
        "    # add to list\n",
        "    lines.append(line)\n",
        "    return lines\n",
        "\n",
        "# load vocabulary\n",
        "vocab_filename = '/content/gdrive/My Drive/txt_sentoken/vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "# prepare negative reviews\n",
        "directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "negative_lines = process_docs(directory_neg, vocab)\n",
        "save_list(negative_lines, '/content/gdrive/My Drive/txt_sentoken/negative.txt')\n",
        "# prepare positive reviews\n",
        "directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "positive_lines = process_docs(directory_pos, vocab)\n",
        "save_list(positive_lines, '/content/gdrive/My Drive/txt_sentoken/positive.txt')"
      ],
      "metadata": {
        "id": "wzhKAwR6R1XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the saved vocabulary in Negative file\n",
        "vocab_filename_neg = '/content/gdrive/My Drive/txt_sentoken/negative.txt'\n",
        "vocab_neg = load_doc(vocab_filename_neg)\n",
        "vocab_neg = vocab_neg.split()\n",
        "vocab_neg = set(vocab_neg)\n",
        "print(len(vocab_neg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQYleyv4r0U2",
        "outputId": "22ff7e07-1186-451d-9b71-febc530d8459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the saved vocabulary in Positive file\n",
        "vocab_filename_pos = '/content/gdrive/My Drive/txt_sentoken/positive.txt'\n",
        "vocab_pos = load_doc(vocab_filename_pos)\n",
        "vocab_pos = vocab_pos.split()\n",
        "vocab_pos = set(vocab_pos)\n",
        "print(len(vocab_pos))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmwtf5tksDl_",
        "outputId": "1ebec44c-eeef-48b0-f807-cfce61451c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project: Develop a Neural Bag-of-Words Model for Sentiment Analysis\n",
        "\n",
        "The evaluation of movie review text is a classification problem often called sentiment analysis.\n",
        "\n",
        "A popular technique for developing\n",
        "sentiment analysis models is to use a bag-of-words model that transforms documents into vectors where each word in the document is assigned a score.\n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "- How to prepare the review text data for modeling with a restricted vocabulary.\n",
        "- How to use the bag-of-words model to prepare train and test data.\n",
        "- How to develop a Multilayer Perceptron bag-of-words model and use it to make predictions on new review text data.\n",
        "\n",
        "This tutorial is divided into the following parts:\n",
        "1. Movie Review Dataset\n",
        "2. Data Preparation\n",
        "3. Bag-of-Words Representation\n",
        "4. Sentiment Analysis Models\n",
        "5. Comparing Word Scoring Methods\n",
        "6. Predicting Sentiment for New Reviews\n",
        "\n",
        "**Data Preparation**\n",
        "\n",
        "Note: The preparation of the movie review dataset was first described in Chapter 9 (previouse cells).\n",
        "\n",
        "In this section, we will look at 3 things:\n",
        "1. Separation of data into training and test sets.\n",
        "2. Loading and cleaning the data to remove punctuation and numbers.\n",
        "3. Defining a vocabulary of preferred words.\n",
        "\n"
      ],
      "metadata": {
        "id": "gWKb-2T5B_ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split into Train and Test Sets**\n",
        "\n",
        "This is a 90% train, 10% split of the data. \n",
        "\n",
        "The split can\n",
        "be imposed easily by using the filenames of the reviews where reviews named 000 to 899 are for\n",
        "training data and reviews named 900 onwards are for testing the model.\n",
        "\n",
        "**Loading and Cleaning Reviews**\n",
        "\n",
        "The text data is already pretty clean, so not much preparation is required. \n",
        "Without getting too much into the details, we will prepare the data using the following method:    ---------------> *clean_doc() function*\n",
        "- Split tokens on white space.\n",
        "- Remove all punctuation from words.\n",
        "- Remove all words that are not purely comprised of alphabetical characters.\n",
        "- Remove all words that are known stop words.\n",
        "- Remove all words that have a length <= 1 character.\n",
        "\n",
        "**Define a Vocabulary**\n",
        "\n",
        "It is important to define a vocabulary of known words when using a bag-of-words model. \n",
        "\n",
        "The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive.\n",
        "\n",
        "We can develop a vocabulary as a Counter, which is a dictionary mapping of words and their count that allows us to easily update and query.\n",
        "\n",
        "Each document can be added to the\n",
        "counter (a new function called add *doc_to_vocab()*) and we can step over all of the reviews in\n",
        "the negative directory and then the positive directory (a new function called *process_docs()*).\n",
        "\n",
        "Running the example shows that we have a vocabulary of 44,276 words. \n",
        "\n",
        "We also can see a sample of the top 50 most used words in the movie reviews. \n",
        "\n",
        "Note that this vocabulary was\n",
        "constructed based on only those reviews in the **training** dataset.\n",
        "\n",
        "The complete example is listed below."
      ],
      "metadata": {
        "id": "BPpy9BASuzq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "    # load doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # update counts\n",
        "    vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "    # skip any reviews in the test set - Seperating Training and Test sets\n",
        "        if filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # add doc to vocab\n",
        "        add_doc_to_vocab(path, vocab)\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "\n",
        "process_docs(directory_pos, vocab)\n",
        "process_docs(directory_neg, vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whmp6pHxuKmH",
        "outputId": "c6eedec6-1ec6-4e78-d2ee-01a64241e7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can step through the vocabulary and remove all words that have a low occurrence, such\n",
        "# as only being used once or twice in all reviews.\n",
        "\n",
        "# keep tokens with a min occurrence\n",
        "min_occurrence = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
        "print(len(tokens))\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, '/content/gdrive/My Drive/txt_sentoken/vocab_train.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7wjoKcuuKj2",
        "outputId": "e78ce6fc-dd2d-4099-bcf1-8ea292a99d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "    # load doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # update counts\n",
        "    vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "    # skip any reviews in the test set - Seperating Training and Test sets\n",
        "        if filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # add doc to vocab\n",
        "        add_doc_to_vocab(path, vocab)\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "    # convert lines to a single blob of text\n",
        "    data = '\\n'.join(lines)\n",
        "    # open file\n",
        "    file = open(filename, 'w')\n",
        "    # write text\n",
        "    file.write(data)\n",
        "    # close file\n",
        "    file.close()\n",
        "\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "\n",
        "process_docs(directory_pos, vocab)\n",
        "process_docs(directory_neg, vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "\n",
        "# keep tokens with a min occurrence\n",
        "min_occurrence = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
        "print('\\n\\nkeep tokens with a min occurrence:\\n',len(tokens))\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, '/content/gdrive/My Drive/txt_sentoken/vocab_train.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi-AqZ-UuKhb",
        "outputId": "d63ba7a2-b16b-4ed6-c105-64a1cf079c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n",
            "\n",
            "\n",
            "keep tokens with a min occurrence:\n",
            " 25767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag-of-Words Representation**\n",
        "\n",
        "First, we need a function to prepare one document. Below lists the function\n",
        "*doc_to_line()* that will load a document, clean it, filter out tokens not in the vocabulary, then return the document as a string of white space separated tokens.\n",
        "\n",
        "Next, we need a function to work through all documents in a directory (such as pos and neg) to convert the documents into lines.\n",
        "\n",
        "*process_docs()* function that does\n",
        "just this, expecting a directory name and a vocabulary set as input arguments and returning a list of processed documents.\n",
        "\n",
        "We can call the *process_docs()* consistently for positive and negative reviews to construct a dataset of review text and their associated output labels, **0 for negative and 1 for positive**.\n",
        "\n",
        "The *load_clean_dataset()* function below implements this behavior."
      ],
      "metadata": {
        "id": "fv-4Xfv03xNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "    # load documents\n",
        "    directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "    directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "\n",
        "    neg = process_docs(directory_neg, vocab)\n",
        "    pos = process_docs(directory_pos, vocab)\n",
        "    docs = neg + pos\n",
        "    # prepare labels\n",
        "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "    return docs, labels"
      ],
      "metadata": {
        "id": "ZxltkbukuKeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we need to load the vocabulary and turn it into a set for use in cleaning reviews.\n",
        "\n",
        "We can put all of this together, reusing the loading and cleaning functions developed in previous sections. \n",
        "\n",
        "The complete example is listed below, demonstrating how to prepare the\n",
        "positive and negative reviews from the training dataset."
      ],
      "metadata": {
        "id": "idHRAtfe592i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "    # load the doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # filter by vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    return ' '.join(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "    lines = list()\n",
        "    for filename in listdir(directory):\n",
        "        # skip any reviews in the test set\n",
        "        if filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load and clean the doc\n",
        "        line = doc_to_line(path, vocab)\n",
        "        # add to list\n",
        "        lines.append(line)\n",
        "    return lines\n",
        "\n",
        "directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "    # load documents\n",
        "    neg = process_docs(directory_neg, vocab)\n",
        "    pos = process_docs(directory_pos, vocab)\n",
        "    docs = neg + pos\n",
        "    # prepare labels\n",
        "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "    return docs, labels\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = '/content/gdrive/My Drive/txt_sentoken/vocab_train.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "# load all training reviews\n",
        "docs, labels = load_clean_dataset(vocab)\n",
        "\n",
        "# summarize what we have\n",
        "# Running this example loads and cleans the review text and returns the labels.\n",
        "\n",
        "print(len(docs), len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtbVyxasuKcG",
        "outputId": "f840ed04-3f46-4176-c2d5-0edfa1e874b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1800 1800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Movie Reviews to Bag-of-Words Vectors**\n",
        "\n",
        "We will use the Keras API to convert reviews to encoded document vectors. \n",
        "\n",
        "Keras provides the Tokenizer class that can do some of the cleaning and vocab definition tasks that we took care of in the previous section. \n",
        "\n",
        "It is better to do this ourselves to know exactly what was done\n",
        "and why. Nevertheless, the Tokenizer class is convenient and will easily transform documents into encoded vectors. \n",
        "\n",
        "First, the Tokenizer must be created, then fit on the text documents\n",
        "in the training dataset. \n",
        "\n",
        "In this case, these are the aggregation of the positive lines and negative lines arrays developed in the previous section.\n",
        "\n"
      ],
      "metadata": {
        "id": "J_hpXbZe_4NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer() \n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "379e7i6fuKZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This process determines a consistent way to convert the vocabulary to a fixed-length vector\n",
        "with 25,768 elements, which is the total number of words in the vocabulary file vocab.txt.\n",
        "\n",
        "Next, documents can then be encoded using the Tokenizer by calling *texts_to_matrix()*. \n",
        "The function takes both a list of documents to encode and an encoding mode, which is the method used to score words in the document. \n",
        "\n",
        "Here we specify *freq* to score words based on their frequency in the document. This can be used to encode the loaded training and test data, for example:"
      ],
      "metadata": {
        "id": "HUDve5O2AxmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This encodes all of the positive and negative reviews in the training dataset.\n",
        "# encode data\n",
        "\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')"
      ],
      "metadata": {
        "id": "6wOsjfDIuKWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the *process_docs()* function from the previous section needs to be modified to selectively process reviews in the test or train dataset. \n",
        "\n",
        "We support the loading of both the training and test datasets by adding an ***is_train*** argument and using that to decide what review file names to skip."
      ],
      "metadata": {
        "id": "-ww0nVbSBGo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "    lines = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "        # skip any reviews in the test set\n",
        "        if is_train and filename.startswith('cv9'):\n",
        "            continue\n",
        "        if not is_train and not filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load and clean the doc\n",
        "        line = doc_to_line(path, vocab)\n",
        "        # add to list\n",
        "        lines.append(line)\n",
        "    return lines"
      ],
      "metadata": {
        "id": "d8es19j7BqCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarly, the load_clean_dataset() dataset must be updated to load either train or test\n",
        "# data and ensure it returns an NumPy array.\n",
        "\n",
        "\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab):\n",
        "    # load documents\n",
        "    directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "    directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "\n",
        "    neg = process_docs(directory_neg, vocab)\n",
        "    pos = process_docs(directory_pos, vocab)\n",
        "    docs = neg + pos\n",
        "    # prepare labels\n",
        "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "    return docs, labels\n"
      ],
      "metadata": {
        "id": "Q1dabmwkBxKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Because there are lots of files to be read in memory, I have uploaded the folders in \n",
        "# Google Drive, so I have to mount it first, then read the data\n",
        "\n",
        "# In this method the Authentication of google drive will open a new window.\n",
        "# There are also other ways to inform the Auth to google in program text\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')     # Mounting Google Drive in Colab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHZ7vZtiRy_g",
        "outputId": "abb7b5f0-3f72-42eb-848e-122e8ad8b3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can put all of this together in a single example.\n",
        "\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "    # load the doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # filter by vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "    lines = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "        # skip any reviews in the test set\n",
        "        if is_train and filename.startswith('cv9'):\n",
        "            continue\n",
        "        if not is_train and not filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load and clean the doc\n",
        "        line = doc_to_line(path, vocab)\n",
        "        # add to list\n",
        "        lines.append(line)\n",
        "    return lines\n",
        "\n",
        "# load and clean a dataset\n",
        "\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "    # load documents\n",
        "    directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "    directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "\n",
        "    neg = process_docs(directory_neg, vocab, is_train)\n",
        "    pos = process_docs(directory_pos, vocab, is_train)\n",
        "    docs = neg + pos\n",
        "    # prepare labels\n",
        "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "    return docs, labels\n",
        "\n",
        "\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = '/content/gdrive/My Drive/txt_sentoken/vocab_train.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "\n",
        "print(Xtrain.shape, Xtest.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjGHkA2YFqSX",
        "outputId": "e8a096de-65a6-48f1-aabc-4fb43bcce474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1800, 25768) (200, 25768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis Models**\n",
        "\n",
        "In this section, we will develop Multilayer Perceptron (MLP) models to classify encoded documents as either positive or negative. \n",
        "\n",
        "The models will be simple feedforward network models with fully connected layers called Dense in the Keras deep learning library. \n",
        "\n",
        "This section is\n",
        "divided into 3 sections:\n",
        "1. First sentiment analysis model\n",
        "2. Comparing word scoring modes\n",
        "3. Making a prediction for new reviews\n",
        "\n",
        "The model\n",
        "will have an input layer that equals the number of words in the vocabulary, and in turn the length of the input documents. \n",
        "\n",
        "We can store this in a new variable called n words, as follows:"
      ],
      "metadata": {
        "id": "CM2aA_kkTkgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = Xtest.shape[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItTIdGotKnxt",
        "outputId": "1f52c5f7-ee2b-46e3-b62c-20b51c49ef3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25768"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now define the network.\n",
        "\n",
        "We will use a single hidden layer with 50 neurons and a rectified linear activation function. \n",
        "\n",
        "The output layer is a single neuron with a sigmoid activation function for predicting 0 for negative and 1 for positive reviews. \n",
        "\n",
        "The network will be trained using the efficient Adam implementation of gradient descent and the binary cross-entropy loss function, suited to binary classification problems. \n",
        "\n",
        "We will keep track of accuracy when training and evaluating the model."
      ],
      "metadata": {
        "id": "cXl_FdxTU3dS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "def define_model(n_words):\n",
        "    # define network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # compile network\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    # summarize defined model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ],
      "metadata": {
        "id": "nIv0HSMxKnvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we can fit the model on the training data; in this case, the model is small and is easily\n",
        "# fit in 10 epochs.\n",
        "\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "\n",
        "\n",
        "# Finally, once the model is trained, we can evaluate its performance by making predictions in\n",
        "# the test dataset and printing the accuracy.\n",
        "\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "metadata": {
        "id": "SlbALqraKntG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The complete example is listed below.\n",
        "\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "    # load the doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # filter by vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "    lines = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "        if is_train and filename.startswith('cv9'):\n",
        "            continue\n",
        "        if not is_train and not filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load and clean the doc\n",
        "        line = doc_to_line(path, vocab)\n",
        "        # add to list\n",
        "        lines.append(line)\n",
        "    return lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "    # load documents\n",
        "    directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "    directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "    \n",
        "    neg = process_docs(directory_neg, vocab, is_train)\n",
        "    pos = process_docs(directory_pos, vocab, is_train)\n",
        "    docs = neg + pos\n",
        "    # prepare labels\n",
        "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "    return docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        " # define the model\n",
        "def define_model(n_words):\n",
        "    # define network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # compile network\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize defined model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = '/content/gdrive/My Drive/txt_sentoken/vocab_train.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
        "\n",
        "# define the model\n",
        "n_words = Xtest.shape[1]\n",
        "model = define_model(n_words)\n",
        "\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('\\n\\nTest Accuracy: %f' % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lsz5OhDYVyUJ",
        "outputId": "6053f76f-8b07-4fab-e56f-95fcee23ad76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 50)                1288450   \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "57/57 - 1s - loss: 0.6918 - accuracy: 0.5578 - 1s/epoch - 24ms/step\n",
            "Epoch 2/10\n",
            "57/57 - 1s - loss: 0.6828 - accuracy: 0.8583 - 877ms/epoch - 15ms/step\n",
            "Epoch 3/10\n",
            "57/57 - 1s - loss: 0.6646 - accuracy: 0.7689 - 869ms/epoch - 15ms/step\n",
            "Epoch 4/10\n",
            "57/57 - 1s - loss: 0.6362 - accuracy: 0.9150 - 867ms/epoch - 15ms/step\n",
            "Epoch 5/10\n",
            "57/57 - 1s - loss: 0.5980 - accuracy: 0.9272 - 879ms/epoch - 15ms/step\n",
            "Epoch 6/10\n",
            "57/57 - 1s - loss: 0.5538 - accuracy: 0.9411 - 867ms/epoch - 15ms/step\n",
            "Epoch 7/10\n",
            "57/57 - 1s - loss: 0.5066 - accuracy: 0.9461 - 875ms/epoch - 15ms/step\n",
            "Epoch 8/10\n",
            "57/57 - 1s - loss: 0.4590 - accuracy: 0.9528 - 848ms/epoch - 15ms/step\n",
            "Epoch 9/10\n",
            "57/57 - 1s - loss: 0.4136 - accuracy: 0.9606 - 856ms/epoch - 15ms/step\n",
            "Epoch 10/10\n",
            "57/57 - 1s - loss: 0.3723 - accuracy: 0.9644 - 851ms/epoch - 15ms/step\n",
            "\n",
            "\n",
            "Test Accuracy: 87.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, to_file='model.png', show_shapes=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "O33i5PcSXprp",
        "outputId": "cd9e045f-5b06-4430-87fd-fb7347b25684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing Word Scoring Methods**\n",
        "\n",
        "The *texts_to_matrix()* function for the Tokenizer in the Keras API provides 4 different methods for scoring words; \n",
        "\n",
        "they are:\n",
        "\n",
        "- **binary** Where words are marked as present (1) or absent (0).\n",
        "- **count** Where the occurrence count for each word is marked as an integer.\n",
        "- **tfidf** Where each word is scored based on their frequency, where words that are common across all documents are penalized.\n",
        "- **freq** Where words are scored based on their frequency of occurrence within the document.\n",
        "\n",
        "We can evaluate the skill of the model developed in the previous section fit using each of the 4 supported word scoring modes.\n",
        "\n",
        "The function *prepare_data()* implements this behavior given lists of train and\n",
        "test documents.\n",
        "\n",
        "\n",
        "We also need a function to **evaluate** the MLP given a specific encoding of the data.\n",
        "\n",
        "The function below, named *evaluate_mode()*, takes encoded documents and evaluates the MLP by training it on the train set and estimating skill on the test set 10 times and returns a list of the accuracy scores across all of these runs.\n",
        "\n"
      ],
      "metadata": {
        "id": "l0xBubt1a2qg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare bag-of-words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "    # create the tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "    # fit the tokenizer on the documents\n",
        "    tokenizer.fit_on_texts(train_docs)\n",
        "    # encode training data set\n",
        "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "    # encode training data set\n",
        "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "    return Xtrain, Xtest"
      ],
      "metadata": {
        "id": "3U-ij4-Aaa4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate a neural network model\n",
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
        "    scores = list()\n",
        "    n_repeats = 30\n",
        "    n_words = Xtest.shape[1]\n",
        "    for i in range(n_repeats):\n",
        "        # define network\n",
        "        model = Sequential()\n",
        "        model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        # compile network\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        # fit network\n",
        "        model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "        # evaluate\n",
        "        loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "        scores.append(acc)\n",
        "        print('%d accuracy: %s' % ((i+1), acc))\n",
        "    return scores"
      ],
      "metadata": {
        "id": "BHMfuIWsaa14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The complete example is listed below.\n",
        "\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from pandas import DataFrame\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "    # load the doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # filter by vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "    lines = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "        if is_train and filename.startswith('cv9'):\n",
        "            continue\n",
        "        if not is_train and not filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load and clean the doc\n",
        "        line = doc_to_line(path, vocab)\n",
        "        # add to list\n",
        "        lines.append(line)\n",
        "    return lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "    # load documents\n",
        "    directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "    directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "    \n",
        "    neg = process_docs(directory_neg, vocab, is_train)\n",
        "    pos = process_docs(directory_pos, vocab, is_train)\n",
        "    docs = neg + pos\n",
        "    # prepare labels\n",
        "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "    return docs, labels\n",
        "\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "    # define network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # compile network\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# evaluate a neural network model\n",
        "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
        "    scores = list()\n",
        "    n_repeats = 10         # Numbers to repeat------------------------------------------#\n",
        "    n_words = Xtest.shape[1]\n",
        "    for i in range(n_repeats):\n",
        "        # define network\n",
        "        model = define_model(n_words)\n",
        "        # fit network\n",
        "        model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
        "        # evaluate\n",
        "        _, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "        scores.append(acc)\n",
        "        print('%d accuracy: %s' % ((i+1), acc))\n",
        "    return scores\n",
        "\n",
        "# prepare bag-of-words encoding of docs\n",
        "def prepare_data(train_docs, test_docs, mode):\n",
        "    # create the tokenizer\n",
        "    tokenizer = Tokenizer()\n",
        "    # fit the tokenizer on the documents\n",
        "    tokenizer.fit_on_texts(train_docs)\n",
        "    # encode training data set\n",
        "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
        "    # encode training data set\n",
        "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
        "    return Xtrain, Xtest\n",
        "\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = '/content/gdrive/My Drive/txt_sentoken/vocab_train.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "\n",
        "# run experiment\n",
        "modes = ['binary', 'count', 'tfidf', 'freq']\n",
        "results = DataFrame()\n",
        "for mode in modes:\n",
        "    # prepare data for mode\n",
        "    Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
        "    # evaluate model on data for mode\n",
        "    results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
        "\n",
        "# summarize results\n",
        "print(results.describe())\n",
        "# plot results\n",
        "results.boxplot()\n",
        "pyplot.title('Accuracy vs Different Word Scoring Methods.')\n",
        "pyplot.ylabel('Accuracy')\n",
        "pyplot.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UFWA-TFCaavh",
        "outputId": "1d54da78-763c-45d1-9536-e9c69ba994df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 accuracy: 0.925000011920929\n",
            "2 accuracy: 0.925000011920929\n",
            "3 accuracy: 0.9300000071525574\n",
            "4 accuracy: 0.9350000023841858\n",
            "5 accuracy: 0.9200000166893005\n",
            "6 accuracy: 0.9399999976158142\n",
            "7 accuracy: 0.9350000023841858\n",
            "8 accuracy: 0.9350000023841858\n",
            "9 accuracy: 0.9350000023841858\n",
            "10 accuracy: 0.925000011920929\n",
            "1 accuracy: 0.8949999809265137\n",
            "2 accuracy: 0.8999999761581421\n",
            "3 accuracy: 0.8949999809265137\n",
            "4 accuracy: 0.9100000262260437\n",
            "5 accuracy: 0.8999999761581421\n",
            "6 accuracy: 0.8999999761581421\n",
            "7 accuracy: 0.8949999809265137\n",
            "8 accuracy: 0.9049999713897705\n",
            "9 accuracy: 0.8949999809265137\n",
            "10 accuracy: 0.8899999856948853\n",
            "1 accuracy: 0.8899999856948853\n",
            "2 accuracy: 0.8799999952316284\n",
            "3 accuracy: 0.875\n",
            "4 accuracy: 0.8849999904632568\n",
            "5 accuracy: 0.8849999904632568\n",
            "6 accuracy: 0.8999999761581421\n",
            "7 accuracy: 0.8650000095367432\n",
            "8 accuracy: 0.8899999856948853\n",
            "9 accuracy: 0.8799999952316284\n",
            "10 accuracy: 0.8650000095367432\n",
            "1 accuracy: 0.8650000095367432\n",
            "2 accuracy: 0.8700000047683716\n",
            "3 accuracy: 0.8799999952316284\n",
            "4 accuracy: 0.8700000047683716\n",
            "5 accuracy: 0.875\n",
            "6 accuracy: 0.875\n",
            "7 accuracy: 0.8700000047683716\n",
            "8 accuracy: 0.8700000047683716\n",
            "9 accuracy: 0.8700000047683716\n",
            "10 accuracy: 0.8700000047683716\n",
            "          binary      count      tfidf       freq\n",
            "count  10.000000  10.000000  10.000000  10.000000\n",
            "mean    0.930500   0.898500   0.881500   0.871500\n",
            "std     0.006433   0.005798   0.011068   0.004116\n",
            "min     0.920000   0.890000   0.865000   0.865000\n",
            "25%     0.925000   0.895000   0.876250   0.870000\n",
            "50%     0.932500   0.897500   0.882500   0.870000\n",
            "75%     0.935000   0.900000   0.888750   0.873750\n",
            "max     0.940000   0.910000   0.900000   0.880000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.boxplot()\n",
        "pyplot.title('Accuracy vs Different Word Scoring Methods.')\n",
        "pyplot.ylabel('Accuracy')\n",
        "pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "RkSazR1Xjp1O",
        "outputId": "11d17f3b-8637-490b-c85d-4aa1349a1911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x576 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NG1-sqedjsUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Nectxnpqki_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting Sentiment for New Reviews**\n",
        "\n",
        "Predicting the sentiment of new reviews involves following the same steps used to prepare the test data. \n",
        "\n",
        "Specifically, \n",
        "- loading the text, \n",
        "- cleaning the document, \n",
        "- filtering tokens by the chosen vocabulary, \n",
        "- converting the remaining tokens to a line, \n",
        "- encoding it using the Tokenizer, \n",
        "- and making a prediction.\n",
        "\n",
        "We can make a prediction of a class value directly with the fit model\n",
        "by calling *predict()* that will return an integer of 0 for a negative review and 1 for a positive\n",
        "review.\n",
        "\n",
        "All of these steps can be put into a new function called *predict_sentiment()* that requires the review text, the vocabulary, the tokenizer, and the fit model and returns the predicted sentiment and an associated percentage or confidence-like output.\n",
        "\n"
      ],
      "metadata": {
        "id": "yySt3_ocmU5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for making predictions for new reviews.\n",
        "\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "    # clean\n",
        "    tokens = clean_doc(review)\n",
        "    # filter by vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    # convert to line\n",
        "    line = ' '.join(tokens)\n",
        "    # encode\n",
        "    encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "    # predict sentiment\n",
        "    yhat = model.predict(encoded, verbose=0)\n",
        "    # retrieve predicted percentage and label\n",
        "    percent_pos = yhat[0,0]\n",
        "    if round(percent_pos) == 0:\n",
        "        return (1-percent_pos), 'NEGATIVE'\n",
        "    return percent_pos, 'POSITIVE'"
      ],
      "metadata": {
        "id": "HaIK0PEonMjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now make predictions for new review texts. Below is an example with both a clearly *positive* and a clearly *negative* review using the simple MLP developed above with the frequency word scoring mode."
      ],
      "metadata": {
        "id": "BGelTjcPncb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test positive text\n",
        "text = 'Best movie ever! It was great, I recommend it.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "\n",
        "\n",
        "# test negative text\n",
        "text = 'This is a bad movie.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
      ],
      "metadata": {
        "id": "zuTY5J5CnT49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pulling this all together, the complete example for making predictions for new reviews is\n",
        "# listed below.\n",
        "\n",
        "\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# load doc, clean and return line of tokens\n",
        "def doc_to_line(filename, vocab):\n",
        "    # load the doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # filter by vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "    lines = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "    # skip any reviews in the test set\n",
        "        if is_train and filename.startswith('cv9'):\n",
        "            continue\n",
        "        if not is_train and not filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load and clean the doc\n",
        "        line = doc_to_line(path, vocab)\n",
        "        # add to list\n",
        "        lines.append(line)\n",
        "    return lines\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "    # load documents\n",
        "    directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "    directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "    \n",
        "    neg = process_docs(directory_neg, vocab, is_train)\n",
        "    pos = process_docs(directory_pos, vocab, is_train)\n",
        "    docs = neg + pos\n",
        "    # prepare labels\n",
        "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "    return docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "# define the model\n",
        "def define_model(n_words):\n",
        "    # define network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # compile network\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize defined model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model\n",
        "\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, model):\n",
        "    # clean\n",
        "    tokens = clean_doc(review)\n",
        "    # filter by vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    # convert to line\n",
        "    line = ' '.join(tokens)\n",
        "    # encode\n",
        "    encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
        "    # predict sentiment\n",
        "    yhat = model.predict(encoded, verbose=0)\n",
        "    # retrieve predicted percentage and label\n",
        "    percent_pos = yhat[0,0]\n",
        "    if round(percent_pos) == 0:\n",
        "        return (1-percent_pos), 'NEGATIVE'\n",
        "    return percent_pos, 'POSITIVE'\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = '/content/gdrive/My Drive/txt_sentoken/vocab_train.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# encode data\n",
        "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
        "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
        "\n",
        "# define network\n",
        "n_words = Xtrain.shape[1]\n",
        "model = define_model(n_words)\n",
        "\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "\n",
        "# test positive text\n",
        "text = 'Best movie ever! It was great, I recommend it.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('\\n\\nReview: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "\n",
        "# test negative text\n",
        "text = 'This is a bad movie.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
        "print('\\n\\nReview: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sscohgI0n8uU",
        "outputId": "9351b0bc-a17a-4a8d-9a42-d3150f3af5e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_41\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_82 (Dense)            (None, 50)                1288450   \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,288,501\n",
            "Trainable params: 1,288,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "57/57 - 1s - loss: 0.4845 - accuracy: 0.7644 - 733ms/epoch - 13ms/step\n",
            "Epoch 2/10\n",
            "57/57 - 0s - loss: 0.0660 - accuracy: 0.9911 - 222ms/epoch - 4ms/step\n",
            "Epoch 3/10\n",
            "57/57 - 0s - loss: 0.0175 - accuracy: 0.9994 - 231ms/epoch - 4ms/step\n",
            "Epoch 4/10\n",
            "57/57 - 0s - loss: 0.0089 - accuracy: 1.0000 - 227ms/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "57/57 - 0s - loss: 0.0055 - accuracy: 1.0000 - 241ms/epoch - 4ms/step\n",
            "Epoch 6/10\n",
            "57/57 - 0s - loss: 0.0035 - accuracy: 1.0000 - 231ms/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "57/57 - 0s - loss: 0.0022 - accuracy: 1.0000 - 235ms/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "57/57 - 0s - loss: 0.0015 - accuracy: 1.0000 - 227ms/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "57/57 - 0s - loss: 0.0011 - accuracy: 1.0000 - 227ms/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "57/57 - 0s - loss: 8.4566e-04 - accuracy: 1.0000 - 238ms/epoch - 4ms/step\n",
            "\n",
            "\n",
            "Review: [Best movie ever! It was great, I recommend it.]\n",
            "Sentiment: POSITIVE (58.302%)\n",
            "\n",
            "\n",
            "Review: [This is a bad movie.]\n",
            "Sentiment: NEGATIVE (62.448%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section lists some extensions if you are looking to get more out of this tutorial.\n",
        "- **Manage Vocabulary**. Explore using a larger or smaller vocabulary. Perhaps you can get better performance with a smaller set of words.\n",
        "- **Tune the Network Topology**. Explore alternate network topologies such as deeper or wider networks. Perhaps you can get better performance with a more suited network.\n",
        "- **Use Regularization**. Explore the use of regularization techniques, such as dropout. Perhaps you can delay the convergence of the model and achieve better test set performance.\n",
        "- **More Data Cleaning**. Explore more or less cleaning of the review text and see how it impacts the model skill.\n",
        "- **Training Diagnostics**. Use the test dataset as a validation dataset during training and create plots of train and test loss. Use these diagnostics to tune the batch size and number of training epochs.\n",
        "- **Trigger Words**. Explore whether there are speci-c words in reviews that are highly predictive of the sentiment.\n",
        "- **Use Bigrams**. Prepare the model to score bigrams of words and evaluate the performance under different scoring schemes.\n",
        "- **Truncated Reviews**. Explore how using a truncated version of the movie reviews results impacts model skill, try truncating the start, end and middle of reviews.\n",
        "- **Ensemble Models**. Create models with different word scoring schemes and see if using ensembles of the models results in improves to model skill.\n",
        "- **Real Reviews**. Train a final model on all data and evaluate the model on real movie reviews taken from the internet."
      ],
      "metadata": {
        "id": "JhryhBIfq5sL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**خلاصه فرآیند مدل سازی تشخیص احساسات با روش کیسه کلمات**\n",
        "\n",
        "تشخیص احساس در مورد نظرات افراد دررابطه با فیلم ها می باشد که هزار نظر مثبت در فولدر نظرات مثبت و هزار نظر منفی در فولدر منفی ذخیره شده است\n",
        "\n",
        "هدف این است که با آموزش این کلمات و متون به ماشین نظرات جدید را بتوانیم دسته بندی کنیم و در واقع خروجی مدل بصورت باینری خواهد بود که صفر برای نظر منفی و یک برای نظر مثبت خواهد بود\n",
        "\n",
        "1.ابتدا کلیه کلمات از متن های مثبت و منفی خوانده و پس از تمیز کاری در فایلی ذخیره می شوند تا بعنوان مرجع مورد استفاده قرار گیرد\n",
        "\n",
        "2.در گام دوم متن های فولدرهای مثبت ومنفی خوانده می شوند و اگر کلمات آنها در مرجع موجود بود در یک خط با یک فاصله اسپیس کنار هم قرار می گیرند و داده های منفی با صفر و مثبت با یک در ستون مجزا لیبل گذاری می شوند \n",
        "\n",
        "3.در گام سوم داده های آموزش و تست باید جداسازی شوند در این درس بر اساس نام فایل که دارای شماره سریال بود این کار انجام شد و ده درصد داده ها برای تست در نظر گرفته شد\n",
        "\n",
        "4.در مرحله چهارم با استفاده از تابع توکنایزر از کتابخانه کراس از کلمات داده های آموزش آرایه ای تشکیل و آنها را با استفاده از تابع فیت آن تکستس اینکود می کنیم و آبجکتی از توکنایزر ایجاد می شود که برای تبدیل داده های تست نیز مورد استفاده قرار میگیرد\n",
        "\n",
        "5.در مرحله پنجم و پس از تشکیل آبجکت توکنایزر از آرایه کلمات داده های آموزشی در مرحله چهارم داده های تست را نیز با همان آبجکت و تابع فیت آن تکستس اینکود می کنیم\n",
        "\n",
        "6.در گام ششم و پس از تشکیل آرایه های داده های آموزش و تست آنها را به ماتریس های عددی تبدیل می کنیم این کار با استفاده از تابع تکست تو ماتریکس از آبجکت توکنایزر گام چهار انجام می شود\n",
        "\n",
        "7.پس از آماده سازی داده های آموزش و تست کار آماده سازی مدل شبکه عصبی با استفاده از تابع سکوئنشال و تعریف لایه ها و تعداد آنها صورت می گیرد\n",
        "\n",
        "8.در گام هشتم مدل را با داده های آموزشی فیت می نماییم \n",
        "\n",
        "9.پس از فیت کردن با داده های آموزشی مدل را با داده های تست آزمایش می کنیم و نمره ارزیابی آن را بررسی می کنیم\n",
        "\n",
        "10.می توانیم با تغییر آرگومان های مختلف در تشکیل توکنایزر گام چهار یا تشکیل مدل در گام هفتم امتیازات بدست آمده را بررسی کنیم\n",
        "\n",
        "11.با انتخاب بهترین پارامترها در گام دهم می توانیم داده های جدید برای گرفتن پیش بینی نتیجه به مدل ارائه نماییم و کار پیش بینی انجام شود"
      ],
      "metadata": {
        "id": "CdCu_FnPFWvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OO7wRNwu7Ncf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part V Word Embeddings"
      ],
      "metadata": {
        "id": "JUmGTwmlsphA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The Word Embedding Model**\n",
        "\n",
        "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. \n",
        "\n",
        "They are a distributed representation for text that is perhaps one\n",
        "of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
        "\n",
        "After completing this chapter, you will know:\n",
        "- What the word embedding approach for representing text is and how it di\u000bers from other feature extraction methods.\n",
        "- That there are 3 main algorithms for learning a word embedding from text data.\n",
        "- That you can either train a new embedding or use a pre-trained embedding on your natural language processing task."
      ],
      "metadata": {
        "id": "AczUocuFsPuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What Are Word Embeddings?**\n",
        "\n",
        "A word embedding is an approach to provide a dense vector representation of words that capture something about their meaning. \n",
        "\n",
        "Word embeddings are an improvement over simpler bag-of-word model word encoding schemes like word counts and frequencies that result in large and sparse vectors (mostly 0 values) that describe documents but not the meaning of the words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "*What does word embedding mean?*\n",
        "Word embedding is just a fancy way of saying numerical representation of words. A good analogy would be how we use the RGB representation for colors. \n",
        "\n",
        "When in science, we say speed of my car is 45 km/hr we gain a sense of how fast/slow we are driving. If we say my friend is driving at 60 km/hr, we can compare which one of us is going faster.\n",
        "\n",
        "What’s worth more a shoe or a purse? Well, as different as those two objects are, one way to answer that is to compare their prices.\n",
        "\n",
        "Now that we know numerical representation of objects aids in analysis by quantifying a certain quality, the question is *what quality of words do we want to quantify?*\n",
        "\n",
        "The answer to that is, we want to quantify the semantics. We want to represent words in such a manner that it captures its meaning in a way humans do. Not the exact meaning of the word but a contextual one. For example, when I say the word see, we know exactly what action — the context — I’m talking about, even though we might not be able to quote its meaning, the kind we would find in a dictionary, of the top of our head.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "(https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314)\n",
        "\n"
      ],
      "metadata": {
        "id": "VR09mZzxuMwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embedding Algorithms**\n",
        "\n",
        "**1- Embedding Layer**\n",
        "\n",
        "The one hot encoded words are mapped to the word vectors. If a Multilayer Perceptron model is used, then the word vectors are concatenated before being fed as input to the model.\n",
        "\n",
        "This approach of learning an embedding layer requires a lot of training data and can be slow, but will learn an embedding both targeted to the specific text data and the NLP task.\n",
        "\n",
        "**2- Word2Vec**\n",
        "\n",
        "Word2Vec is a statistical method for efficiently learning a standalone word embedding from a text corpus.\n",
        "\n",
        "Additionally, the work involved analysis of the learned vectors and the exploration of vector math on the representations of words. \n",
        "\n",
        "For example, that subtracting the man-ness from *King* and adding women-ness results in the word *Queen*, capturing the analogy king is to queen as\n",
        "man is to woman.\n",
        "\n",
        "Two different learning models were introduced that can be used as part of the Word2Vec approach to learn the word embedding; they are:\n",
        "\n",
        "- Continuous Bag-of-Words, or CBOW model.\n",
        "- Continuous Skip-Gram Model.\n",
        "\n",
        "The CBOW model learns the embedding by predicting the current word based on its context.\n",
        "\n",
        "The continuous skip-gram model learns by predicting the surrounding words given a current word.\n",
        "\n",
        "Both models are focused on learning about words given their local usage context, where the context is defined by a window of neighboring words. \n",
        "\n",
        "This window is a configurable parameter of the model.\n",
        "\n",
        "The size of the sliding window has a strong effect on the resulting vector similarities.\n",
        "\n",
        "Large windows tend to produce more topical similarities [...], while smaller windows tend to produce more functional and syntactic similarities.\n",
        "\n",
        "The key benefit of the approach is that high-quality word embeddings can be learned efficiently (low space and time complexity), allowing larger embeddings to be learned (more dimensions) from much larger corpora of text (billions of words).\n",
        "\n",
        "\n",
        "**3- Glo Ve**\n",
        "\n",
        "The Global Vectors for Word Representation, or GloVe, algorithm is an extension to the Word2Vec method for e\u000eciently learning word vectors.\n",
        "\n",
        "Rather than using a window to define local context, GloVe constructs an explicit word-context or word co-occurrence matrix using statistics across the whole text corpus. \n",
        "\n",
        "The result is a learning model that may result in\n",
        "generally better word embeddings.\n",
        "\n",
        "GloVe, is a new global log-bilinear regression model for the unsupervised learning of word representations that outperforms other models on word analogy, word similarity, and named entity recognition tasks."
      ],
      "metadata": {
        "id": "yhEOUDoO2IwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Word Embeddings**\n",
        "\n",
        "**Learn an Embedding**\n",
        "\n",
        "You may choose to learn a word embedding for your problem. This will require a large amount of text data to ensure that useful embeddings are learned, such as millions or billions of words.\n",
        "\n",
        "You have two main options when training your word embedding:\n",
        "- **Learn it Standalone**, where a model is trained to learn the embedding, which is saved and used as a part of another model for your task later. This is a good approach if you would like to use the same embedding in multiple models.\n",
        "- **Learn Jointly**, where the embedding is learned as part of a large task-specific model. This is a good approach if you only intend to use the embedding on one task.\n",
        "\n",
        "**Reuse an Embedding**\n",
        "\n",
        "It is common for researchers to make pre-trained word embeddings available for free, often under a permissive license so that you can use them on your own academic or commercial projects. For example, both Word2Vec and GloVe word embeddings are available for free download. \n",
        "\n",
        "These can be used on your project instead of training your own embeddings from scratch. You have\n",
        "two main options when it comes to using pre-trained embeddings:\n",
        "- **Static**, where the embedding is kept static and is used as a component of your model. This is a suitable approach if the embedding is a good fit for your problem and gives good\n",
        "results.\n",
        "- **Updated**, where the pre-trained embedding is used to seed the model, but the embedding is updated jointly during the training of the model. This may be a good option if you are looking to get the most out of the model and embedding on your task."
      ],
      "metadata": {
        "id": "EdLPtr8j5aqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How to Develop Word Embeddings with Gensim**\n",
        "\n",
        "\n",
        "Word embeddings are a modern approach for representing text in natural language processing.\n",
        "\n",
        "Embedding algorithms like Word2Vec and GloVe are key to the state-of-the-art results achieved by neural network models on natural language processing problems like machine translation.\n",
        "\n",
        "After completing this tutorial, you\n",
        "will know:\n",
        "- How to train your own Word2Vec word embedding model on text data.\n",
        "- How to visualize a trained word embedding model using Principal Component Analysis.\n",
        "- How to load pre-trained Word2Vec and GloVe word embedding models from Google and Stanford.\n",
        "\n"
      ],
      "metadata": {
        "id": "hTa8zdA-7BKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embeddings**\n",
        "\n",
        "A word embedding is an approach to provide a dense vector representation of words that capture something about their meaning. \n",
        "\n",
        "Word embeddings are an improvement over simpler bag-of-word\n",
        "model word encoding schemes like word counts and frequencies that result in large and sparse vectors (mostly 0 values) that describe documents but not the meaning of the words.\n",
        "\n",
        "In this tutorial, we are going to look at how to use two di\u000berent word embedding methods called Word2Vec by researchers at Google and GloVe by researchers at Stanford.\n",
        "\n",
        "\n",
        "**Gensim Python Library**\n",
        "\n",
        "Gensim is a mature, focused, and efficient suite of NLP tools for topic modeling.\n",
        "\n",
        "It also provides tools for loading pre-trained word embeddings in a few formats and for making use and querying a loaded embedding.\n",
        "\n",
        "**Develop Word2Vec Embedding**\n",
        "\n",
        "Word2Vec is one algorithm for learning a word embedding from a text corpus. \n",
        "\n",
        "There are two main training algorithms that can be used to learn the embedding from text; they are **Continuous Bag-of-Words (CBOW)** and **skip grams**. \n",
        "\n",
        "We will not get into the algorithms other than to say that they generally look at a window of words for each target word to provide context and in turn meaning for words. \n",
        "\n",
        "The approach was developed by Tomas Mikolov, formerly at Google and currently at Facebook.\n",
        "\n",
        "Gensim provides the Word2Vec class for working with a Word2Vec model. Learning a word embedding from text involves loading and organizing the text into sentences and providing them to the constructor of a new *Word2Vec()* instance. \n",
        "\n",
        "For example:\n",
        "\n",
        "sentences = ...\n",
        "\n",
        "model = Word2Vec(sentences)\n",
        "\n",
        "\n",
        "\n",
        "Specifically, each sentence must be tokenized, meaning divided into words and prepared (e.g. perhaps pre-filtered and perhaps converted to a preferred case). \n",
        "\n",
        "The sentences could be text loaded into memory, or an iterator that progressively loads text, required for very large text corpora. \n",
        "\n",
        "There are many parameters on this constructor; a few noteworthy arguments you may wish to configure are:\n",
        "- **size**: (default 100) The number of dimensions of the embedding, e.g. the length of the\n",
        "dense vector to represent each token (word).\n",
        "- **window**: (default 5) The maximum distance between a target word and words around the\n",
        "target word.\n",
        "- **min count**: (default 5) The minimum count of words to consider when training the model;\n",
        "words with an occurrence less than this count will be ignored.\n",
        "- **workers**: (default 3) The number of threads to use while training.\n",
        "- **sg**: (default 0 or CBOW) The training algorithm, either **CBOW** (0) or **skip gram** (1)."
      ],
      "metadata": {
        "id": "PSG-D5zQ734m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-Ai78V9w_6QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "['this', 'is', 'the', 'second', 'sentence'],\n",
        "['yet', 'another', 'sentence'],\n",
        "['one', 'more', 'sentence'],\n",
        "['and', 'the', 'final', 'sentence']]\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "# summarize the loaded model\n",
        "print(model)\n",
        "# summarize vocabulary\n",
        "words = list(model.wv.vocab)\n",
        "print('\\n\\nwords are\\n\\n',words)\n",
        "\n",
        "# print(model.wv.__getitem__('sentence'))\n",
        "# access vector for one word\n",
        "print('\\n\\nword embedding vector Shape:',model.wv.__getitem__('sentence').shape)\n",
        "print('\\n\\nword embedding vector\\n\\n',model.wv.__getitem__('sentence'))\n",
        "# save model\n",
        "model.save('model.bin')\n",
        "# load model\n",
        "new_model = Word2Vec.load('model.bin')\n",
        "print(new_model)"
      ],
      "metadata": {
        "id": "uftF1CAHsntK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f6fa2d-cca5-4326-9967-b6849be0779d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec(vocab=14, size=100, alpha=0.025)\n",
            "\n",
            "\n",
            "words are\n",
            "\n",
            " ['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
            "\n",
            "\n",
            "word embedding vector Shape: (100,)\n",
            "\n",
            "\n",
            "word embedding vector\n",
            "\n",
            " [-0.00408992  0.00233549  0.00245788 -0.00315892 -0.00015476 -0.00019615\n",
            " -0.00400534 -0.00437021 -0.00356167  0.00106739 -0.0026697   0.00081435\n",
            " -0.00198241 -0.0021575  -0.00418156  0.0009296  -0.00373121 -0.00068261\n",
            "  0.00031517  0.00283401 -0.00122963 -0.00483537  0.00444259 -0.00359695\n",
            " -0.00415709 -0.00259186  0.00225392  0.001174    0.00149953 -0.00091013\n",
            " -0.00222917  0.00462779 -0.00251706  0.0007516  -0.00359216  0.00396291\n",
            "  0.00211514 -0.00244991  0.00296053  0.00177733  0.00189665 -0.0026813\n",
            " -0.00275128  0.00477049  0.00030689 -0.00436679  0.00448727  0.00209161\n",
            " -0.0016321  -0.00416475 -0.00225303 -0.0034928   0.00411773  0.00181645\n",
            " -0.00353575 -0.00010728 -0.00370376 -0.00463837 -0.00397267  0.0048064\n",
            " -0.00303444  0.00235117  0.00487285 -0.00351589 -0.00346969  0.00449518\n",
            " -0.00262899  0.00059751  0.0045228  -0.00025577 -0.00185464 -0.00040565\n",
            "  0.00405882 -0.00222669  0.0044264   0.0008603  -0.00240773  0.00061937\n",
            " -0.00117363  0.00188675  0.00367595 -0.00319599  0.00420387 -0.00476647\n",
            "  0.00438284  0.00181443 -0.00215483  0.00026495  0.00277521  0.00204697\n",
            "  0.00171168  0.00348321  0.00183304  0.0013634   0.00250114  0.00406282\n",
            "  0.00294113  0.00078327  0.00191978  0.00435864]\n",
            "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot Word Vectors Using PCA**\n",
        "\n",
        "PCA(Priciple Components Analysis) will give us two components of each vector (from 100 in each vector) that we can use it for drawing a plot (scatterplot).\n",
        "\n",
        "We can retrieve all of the vectors from a trained model as follows:\n",
        "\n",
        "X = model[model.wv.vocab]\n",
        "\n",
        "and use PCA from Scikit Learn"
      ],
      "metadata": {
        "id": "vHWSOZT-86vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "['this', 'is', 'the', 'second', 'sentence'],\n",
        "['yet', 'another', 'sentence'],\n",
        "['one', 'more', 'sentence'],\n",
        "['and', 'the', 'final', 'sentence']]\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "# fit a 2d PCA model to the vectors\n",
        "X = model[model.wv.vocab]\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "# create a scatter plot of the projection\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(model.wv.vocab)\n",
        "for i, word in enumerate(words):\n",
        "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "5RwzNX_YHdtK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "0c0abab8-c73d-413b-c0bc-04e06c2fe650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Google's Word2Vec Embedding**\n",
        "\n",
        "The pre-trained Google Word2Vec model was trained on Google news data (about 100\n",
        "billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors. It is a 1.53 Gigabyte file. \n",
        "\n",
        "You can download it from here:\n",
        "\n",
        "GoogleNews-vectors-negative300.bin.gz.\n",
        "\n",
        "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
        "\n",
        "Unzipped, the binary file (GoogleNews-vectors-negative300.bin) is 3.4 Gigabytes. \n",
        "\n",
        "The Gensim library provides tools to load this file. \n",
        "\n",
        "Specically, you can call the *KeyedVectors.load_word2vec_format()* function to load this model into memory, \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iA4p4-7j-hVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for example:\n",
        "\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "metadata": {
        "id": "fVm8DqRC9gq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example demonstrating arithmetic with Google word vectors.\n",
        "\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "# load the google word2vec model\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
        "# calculate: (king - man) + woman = ?\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n",
        "\n",
        "\n",
        "# Reslt of Above code --------------->  [('queen', 0.7118192315101624)]"
      ],
      "metadata": {
        "id": "gvNZIJtoBuM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Stanford's GloVe Embedding**\n",
        "\n",
        "Like Word2Vec, the GloVe researchers also provide pre-trained word vectors, in this case, a great selection to choose from. \n",
        "\n",
        "You can download the GloVe pre-trained word vectors and load them easily with Gensim.\n",
        "\n",
        "The first step is to convert the GloVe file format to the Word2Vec file\n",
        "format. The only di\u000berence is the addition of a small header line. \n",
        "\n",
        "This can be done by calling the *glove2word2vec()* function. \n",
        "\n",
        "For example Working with the 100-dimensional version of the model, we can convert the file to Word2Vec format as follows:"
      ],
      "metadata": {
        "id": "o2Z1e33dDcc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_input_file = 'glove.6B.100d.txt'\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)"
      ],
      "metadata": {
        "id": "9o7wcYF4BuKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example demonstrating how to load and use GloVe word embeddings.\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# convert glove to word2vec format\n",
        "glove_input_file = 'glove.6B.100d.txt'\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)\n",
        "\n",
        "# load the converted model\n",
        "filename = 'glove.6B.100d.txt.word2vec'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
        "\n",
        "\n",
        "# calculate: (king - man) + woman = ?\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n",
        "\n",
        "\n",
        "# Reslt of Above code --------------->  [('queen', 0.7698540687561035)]"
      ],
      "metadata": {
        "id": "GxnkuIwUD83V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SO2pgouiBuEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How to Learn and Load Word Embeddings in Keras**\n",
        "\n",
        "\n",
        "Two\n",
        "popular examples of methods of learning word embeddings from text include:\n",
        "- Word2Vec.\n",
        "- GloVe.\n",
        "\n",
        "In addition to these carefully designed methods, a word embedding can be learned as part of a deep learning model. \n",
        "\n",
        "This can be a slower approach, but tailors the model to a specific training dataset.\n",
        "\n",
        "**Keras Embedding Layer**\n",
        "\n",
        "Keras offers an Embedding layer that can be used for neural networks on text data. It requires that the input data be integer encoded, so that each word is represented by a unique integer.\n",
        "\n",
        "This data preparation step can be performed using the Tokenizer API also provided with Keras.\n",
        "\n",
        "The Embedding layer is initialized with **random weights** and will learn an embedding for all of the words in the training dataset. \n",
        "\n",
        "It is a flexible layer that can be used in a variety of ways, such as:\n",
        "- It can be used alone to learn a word embedding that can be saved and used in another\n",
        "model later.\n",
        "- It can be used as part of a deep learning model where the embedding is learned along with the model itself.\n",
        "- It can be used to load a pre-trained word embedding model, a type of transfer learning.\n",
        "\n",
        "The Embedding layer is defined as the first hidden layer of a network. It must specify **3 arguments**:\n",
        "\n",
        "- **input dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
        "\n",
        "- **output dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test di\u000berent values for your problem.\n",
        "- **input length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
      ],
      "metadata": {
        "id": "QLWTuTNCEqBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For example, below we define an Embedding layer with a vocabulary of 200 (e.g. integer\n",
        "# encoded words from 0 to 199, inclusive), a vector space of 32 dimensions in which words will be\n",
        "# embedded, and input documents that have 50 words each.\n",
        "\n",
        "e = Embedding(200, 32, input_length=50)"
      ],
      "metadata": {
        "id": "czjdM2tEU7qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5RR1UJvAVEmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of Learning an Embedding**\n",
        "\n",
        "We will define a small problem where we have 10\n",
        "text documents, each with a comment about a piece of work a student submitted. Each text document is classified as positive 1 or negative 0. \n",
        "\n",
        "This is a simple sentiment analysis problem.\n",
        "\n",
        "First, we will define the documents and their class labels."
      ],
      "metadata": {
        "id": "bQVpzNPqU6vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define document\n",
        "\n",
        "from numpy import array\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!',\n",
        "'Weak',\n",
        "'Poor effort!',\n",
        "'not good',\n",
        "'poor work',\n",
        "'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "CP7-eNMoVial"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras provides the one hot() function that creates a\n",
        "# hash of each word as an e\u000ecient integer encoding. We will estimate the vocabulary size of 50,\n",
        "# which is much larger than needed to reduce the probability of collisions from the hash function.\n",
        "\n",
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print(encoded_docs)"
      ],
      "metadata": {
        "id": "o5y5z7EVVpEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The sequences have di\u000berent lengths and Keras prefers inputs to be vectorized and all inputs\n",
        "# to have the same length. We will pad all input sequences to have the length of 4. Again, we can\n",
        "# do this with a built in Keras function, in this case the pad sequences() function.\n",
        "\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print(padded_docs)"
      ],
      "metadata": {
        "id": "jy-FZqKJWDb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to define our Embedding layer as part of our neural network model.\n",
        "\n",
        "The Embedding layer has a vocabulary of 50 and an input length of 4. We will choose a small embedding space of 8 dimensions. The model is a simple binary classification model.\n",
        "\n",
        "Importantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. \n",
        "\n",
        "We flatten this to a one 32-element vector to pass on to the Dense output layer."
      ],
      "metadata": {
        "id": "nhUVqEnRWteU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "XzDcns7GW1RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, we can fit and evaluate the classification model.\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "metadata": {
        "id": "MkcYVfryWaIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The complete code listing is provided below.\n",
        "\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "# define documents\n",
        "docs = ['Well done!',\n",
        "        'Good work',\n",
        "        'Great effort',\n",
        "        'nice work',\n",
        "        'Excellent!',\n",
        "        'Weak',\n",
        "        'Poor effort!',\n",
        "        'not good',\n",
        "        'poor work',\n",
        "        'Could have done better.']\n",
        "# define class labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])\n",
        "# integer encode the documents\n",
        "vocab_size = 50\n",
        "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
        "print('Running the example first prints the integer encoded documents:\\n',encoded_docs)\n",
        "# pad documents to a max length of 4 words\n",
        "max_length = 4\n",
        "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "print('\\n\\nThen the padded versions of each document are printed, making them all uniform length:\\n',padded_docs)\n",
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# summarize the model\n",
        "model.summary()\n",
        "# fit the model\n",
        "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kL899NCW9kX",
        "outputId": "dad49f22-286a-4cf6-b07d-99c79a1a328d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the example first prints the integer encoded documents:\n",
            " [[4, 27], [25, 41], [24, 38], [48, 41], [11], [18], [45, 38], [5, 25], [45, 41], [5, 5, 27, 36]]\n",
            "\n",
            "\n",
            "Then the padded versions of each document are printed, making them all uniform length:\n",
            " [[ 4 27  0  0]\n",
            " [25 41  0  0]\n",
            " [24 38  0  0]\n",
            " [48 41  0  0]\n",
            " [11  0  0  0]\n",
            " [18  0  0  0]\n",
            " [45 38  0  0]\n",
            " [ 5 25  0  0]\n",
            " [45 41  0  0]\n",
            " [ 5  5 27 36]]\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 4, 8)              400       \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 433\n",
            "Trainable params: 433\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_test_function.<locals>.test_function at 0x7f5397c1d8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Accuracy: 89.999998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tips for Cleaning Text for Word Embedding**\n",
        "\n",
        "Tomas Mikolov (one of the developers of Word2Vec, a popular word embedding method) :\n",
        "\n",
        "There is no universal answer. It all depends on what you plan to use the vectors\n",
        "for. \n",
        "\n",
        "In my experience, it is usually good to disconnect (or remove) punctuation from words, and sometimes also convert all characters to lowercase. One can also replace all numbers (possibly greater than some constant) with some single token such as .\n",
        "\n",
        "All these pre-processing steps aim to reduce the vocabulary size without removing any important content (which in some cases may not be true when you lowercase certain words, \n",
        "\n",
        "ie. `Bush' is different than `bush', while `Another' has usually the same sense as `another'). The smaller the vocabulary is, the lower is the memory complexity, and the more robustly are the parameters for the words estimated. \n",
        "\n",
        "You also have to pre-process the test data in the same way.\n",
        "\n",
        "In short, you will understand all this much better if you will run experiments."
      ],
      "metadata": {
        "id": "wJzRxqM7dfSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Yr2StH7xXOHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part VI Text Classiffication\n",
        "\n",
        "**Neural Models for Document Classiffication**\n",
        "\n",
        "Text classiffication describes a general class of problems such as predicting the sentiment of tweets and movie reviews, as well as classifying email as spam or not. \n",
        "\n",
        "Deep learning methods are proving very good at text classiffication, achieving state-of-the-art results on a suite of standard academic benchmark problems.\n",
        "\n",
        "**Overview**\n",
        "\n",
        "This tutorial is divided into the following parts:\n",
        "1. Word Embeddings + CNN = Text classiffication\n",
        "2. Use a Single Layer CNN Architecture\n",
        "3. Dial in CNN Hyperparameters\n",
        "4. Consider Character-Level CNNs\n",
        "5. Consider Deeper CNNs for classiffication"
      ],
      "metadata": {
        "id": "j2MNoSENag4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Embeddings + CNN = Text Classiffication**\n",
        "\n",
        "Neural networks in general o\u000ber better performance than classical\n",
        "linear classiffiers, especially when used with pre-trained word embeddings.\n",
        "\n",
        "Convolutional Neural Networks (CNN) are effective at document Classiffication,\n",
        "namely because they are able to pick out salient features (e.g. tokens or sequences of tokens) in a way that is invariant to their position within the input sequences.\n",
        "\n",
        "The architecture is therefore comprised of three key pieces:\n",
        "\n",
        "- **Word Embedding**: A distributed representation of words where di\u000berent words that have a similar meaning (based on their usage) also have a similar representation.\n",
        "- **Convolutional Model**: A feature extraction model that learns to extract salient features from documents represented using a word embedding.\n",
        "- **Fully Connected Model**: The interpretation of extracted features in terms of a predictive output."
      ],
      "metadata": {
        "id": "oaeLw3nJbeJ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use a Single Layer CNN Architecture**\n",
        "\n",
        "Yoon Kim in his study of the use of pre-trained word vectors for classiffication tasks with Convolutional Neural Networks found that using pre-trained static word vectors does very well.\n",
        "\n",
        "He suggests that pre-trained word embeddings that were trained on very large text corpora, such as the freely available Word2Vec vectors trained on 100 billion tokens from Google news may offer good universal features for use in natural language processing."
      ],
      "metadata": {
        "id": "FzTJzYZQdTsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mcpD9R-6gloL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project: Develop an Embedding + CNN Model for Sentiment Analysis**\n",
        "\n",
        "In this tutorial, you will discover how to develop word embedding models\n",
        "with convolutional neural networks to classify movie reviews. \n",
        "\n",
        "After completing this tutorial, you will know:\n",
        "- How to prepare movie review text data for classiffication with deep learning methods.\n",
        "- How to develop a neural classiffication model with word embedding and convolutional layers.\n",
        "- How to evaluate the developed a neural classiffication model.\n",
        "\n",
        "\n",
        "**Data Preparation**\n",
        "\n",
        "**Note**: The preparation of the movie review dataset was first described in Part IV. \n",
        "\n",
        "In this section, we will look at 3 things:\n",
        "1. Separation of data into training and test sets.\n",
        "2. Loading and cleaning the data to remove punctuation and numbers.\n",
        "3. Defining a vocabulary of preferred words.\n",
        "\n",
        "**Split into Train and Test Sets**\n",
        "\n",
        "we will use the last 100 positive reviews and the last 100 negative reviews\n",
        "as a test set (100 reviews) and the remaining 1,800 reviews as the training dataset. This is a 90% train, 10% split of the data. \n",
        "\n",
        "The split can be imposed easily by using the filenames of the\n",
        "reviews where reviews named 000 to 899 are for training data and reviews named 900 onwards are for test.\n",
        "\n",
        "**Loading and Cleaning Reviews**   -----------> clean_doc(doc) function\n",
        "\n",
        "The text data is already pretty clean; not much preparation is required. Without getting bogged down too much in the details, we will prepare the data using the following way:\n",
        "- Split tokens on white space.\n",
        "- Remove all punctuation from words.\n",
        "- Remove all words that are not purely comprised of alphabetical characters.\n",
        "- Remove all words that are known stop words.\n",
        "- Remove all words that have a length >= 1 character."
      ],
      "metadata": {
        "id": "xpPTCrvBfwx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aqCkYmmM6GJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pulling all of this together, the complete example is listed below.\n",
        "\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n",
        "\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "    # load doc\n",
        "    doc = load_doc(filename)\n",
        "    # clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "    # update counts\n",
        "    vocab.update(tokens)\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "    # skip any reviews in the test set - Seperating Training and Test sets\n",
        "        if filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # add doc to vocab\n",
        "        add_doc_to_vocab(path, vocab)\n",
        "\n",
        "# save list to file\n",
        "def save_list(lines, filename):\n",
        "    # convert lines to a single blob of text\n",
        "    data = '\\n'.join(lines)\n",
        "    # open file\n",
        "    file = open(filename, 'w')\n",
        "    # write text\n",
        "    file.write(data)\n",
        "    # close file\n",
        "    file.close()\n",
        "\n",
        "\n",
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "\n",
        "process_docs(directory_pos, vocab)\n",
        "process_docs(directory_neg, vocab)\n",
        "\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "# print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "\n",
        "# keep tokens with a min occurrence\n",
        "min_occurrence = 2\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
        "print('\\n\\nkeep tokens with a min occurrence:\\n',len(tokens))\n",
        "\n",
        "# save tokens to a vocabulary file\n",
        "save_list(tokens, '/content/gdrive/My Drive/txt_sentoken/vocab_train.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRDFPQX169ML",
        "outputId": "c55e0d3b-72e9-41a0-d4f3-ff19202e702f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44276\n",
            "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n",
            "\n",
            "\n",
            "keep tokens with a min occurrence:\n",
            " 25767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to look at extracting features from the reviews ready for modeling.\n",
        "\n",
        "**Train CNN With Embedding Layer**\n",
        "\n",
        "The real valued vector representation for words can be learned while training the neural network. We can do this in the Keras deep learning library using the Embedding layer. \n",
        "\n",
        "The first step is to load the vocabulary. We will use it to filter out words from movie reviews that we are not interested in.\n",
        "\n",
        "The steps of preparing data and train and test is as above cells in project of Developing Neural Bag-of-Words Model for Sentiment Analysis....So please look there for the comments on functions.\n",
        "\n",
        "Now that the mapping of words to integers has been prepared, we can use it to encode the reviews in the training dataset. We can do that by calling the *texts_to_sequences()* function on the Tokenizer. \n",
        "\n",
        "We also need to ensure that all documents have the same length. This is a\n",
        "requirement of Keras for efficient computation. We could truncate reviews to the smallest size or zero-pad (pad with the value 0) reviews to the maximum length, or some hybrid. \n",
        "\n",
        "In this case, we will pad all reviews to the length of the longest review in the training dataset. \n",
        "\n",
        "First, we can find the longest review using the *max()* function on the training dataset and take its length.\n",
        "\n",
        "We can then call the Keras function *pad_sequences()* to pad the sequences to the maximum length by adding 0 values on the end.\n",
        "\n",
        "\n",
        "The Embedding layer requires the speciffication of the vocabulary\n",
        "size, the size of the real-valued vector space, and the maximum length of input documents. \n",
        "\n",
        "The vocabulary size is the total number of words in our vocabulary, plus one for unknown words.\n",
        "\n",
        "This could be the vocab set length or the size of the vocab within the tokenizer used to integer\n",
        "encode the documents\n",
        "\n",
        "In addition, We will use a 100-dimensional vector space, but you could try other values, such as 50 or 150. \n",
        "\n",
        "Finally, the maximum document length was calculated above in the max length variable used during padding. The complete model definition is listed below including the Embedding layer. \n",
        "\n",
        "We use a Convolutional Neural Network (CNN) as they have proven to be successful at document classification problems. \n",
        "\n",
        "A conservative CNN configuration is used with 32 filters\n",
        "(parallel fields for processing words) and a kernel size of 8 with a rectified linear (relu) activation function. \n",
        "\n",
        "This is followed by a pooling layer that reduces the output of the convolutional layer by half.\n",
        "\n",
        "Next, the 2D output from the CNN part of the model is \n",
        "attened to one long 2D vector to represent the features extracted by the CNN. \n",
        "\n",
        "The back-end of the model is a standard Multilayer Perceptron layers to interpret the CNN features. The output layer uses a sigmoid activation\n",
        "function to output a value between 0 and 1 for the negative and positive sentiment in the review.\n",
        "\n",
        "here is the complete code:"
      ],
      "metadata": {
        "id": "U8VbOMLl7LJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete example of fitting a CNN model with an Embedding input layer.\n",
        "\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # filter out tokens not in vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "    documents = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "        # skip any reviews in the test set\n",
        "        if is_train and filename.startswith('cv9'):\n",
        "            continue\n",
        "        if not is_train and not filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load the doc\n",
        "        doc = load_doc(path)\n",
        "        # clean doc\n",
        "        tokens = clean_doc(doc, vocab)\n",
        "        # add to list\n",
        "        documents.append(tokens)\n",
        "    return documents\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "    # load documents\n",
        "    directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "    directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "\n",
        "    neg = process_docs(directory_neg, vocab, is_train)\n",
        "    pos = process_docs(directory_pos, vocab, is_train)\n",
        "    docs = neg + pos\n",
        "    # prepare labels\n",
        "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "    return docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "    # integer encode\n",
        "    encoded = tokenizer.texts_to_sequences(docs)\n",
        "    # pad sequences\n",
        "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "    return padded\n",
        "\n",
        "# define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "    model.add(Conv1D(32, 8, activation='relu'))\n",
        "    model.add(MaxPooling1D())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # compile network\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize defined model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = '/content/gdrive/My Drive/txt_sentoken/vocab_train.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load training data\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "# define model\n",
        "model = define_model(vocab_size, max_length)\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "# save the model\n",
        "model.save('/content/gdrive/My Drive/txt_sentoken/model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6buAbA6jBki7",
        "outputId": "70d62476-a060-4a65-a7c0-4d544044081e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 25768\n",
            "Maximum length: 1317\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1317, 100)         2576800   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 1310, 32)          25632     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 655, 32)          0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 20960)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                209610    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,812,053\n",
            "Trainable params: 2,812,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "57/57 - 16s - loss: 0.6893 - accuracy: 0.5106 - 16s/epoch - 288ms/step\n",
            "Epoch 2/10\n",
            "57/57 - 0s - loss: 0.5152 - accuracy: 0.7517 - 433ms/epoch - 8ms/step\n",
            "Epoch 3/10\n",
            "57/57 - 0s - loss: 0.1073 - accuracy: 0.9650 - 444ms/epoch - 8ms/step\n",
            "Epoch 4/10\n",
            "57/57 - 0s - loss: 0.0097 - accuracy: 0.9994 - 430ms/epoch - 8ms/step\n",
            "Epoch 5/10\n",
            "57/57 - 0s - loss: 0.0023 - accuracy: 1.0000 - 436ms/epoch - 8ms/step\n",
            "Epoch 6/10\n",
            "57/57 - 0s - loss: 0.0013 - accuracy: 1.0000 - 431ms/epoch - 8ms/step\n",
            "Epoch 7/10\n",
            "57/57 - 0s - loss: 9.0201e-04 - accuracy: 1.0000 - 436ms/epoch - 8ms/step\n",
            "Epoch 8/10\n",
            "57/57 - 0s - loss: 6.9060e-04 - accuracy: 1.0000 - 431ms/epoch - 8ms/step\n",
            "Epoch 9/10\n",
            "57/57 - 0s - loss: 5.4913e-04 - accuracy: 1.0000 - 443ms/epoch - 8ms/step\n",
            "Epoch 10/10\n",
            "57/57 - 0s - loss: 4.4897e-04 - accuracy: 1.0000 - 432ms/epoch - 8ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Model**\n",
        "\n",
        "In this section, we will evaluate the trained model and use it to make predictions on new data.\n",
        "\n",
        "First, we can use the built-in *evaluate()* function to estimate the skill of the model on both the training and test dataset. \n",
        "\n",
        "This requires that we load and encode both the training and test\n",
        "datasets.\n",
        "\n",
        "We can then load the model and evaluate it on both datasets and print the accuracy.\n",
        "\n",
        "New data must then be prepared using the same text encoding and encoding schemes as was used on the training dataset. \n",
        "\n",
        "Once prepared, a prediction can be made by calling the *predict()*\n",
        "function on the model. \n",
        "\n",
        "The function below named *predict_sentiment()* will encode and pad a given movie review text and return a prediction in terms of both the percentage and a label.\n",
        "\n",
        "We can test out this model with two ad hoc movie reviews. \n",
        "\n",
        "The complete example is listed below."
      ],
      "metadata": {
        "id": "pfDxKhMHE28e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # filter out tokens not in vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "    documents = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "        # skip any reviews in the test set\n",
        "        if is_train and filename.startswith('cv9'):\n",
        "            continue\n",
        "        if not is_train and not filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load the doc\n",
        "        doc = load_doc(path)\n",
        "        # clean doc\n",
        "        tokens = clean_doc(doc, vocab)\n",
        "        # add to list\n",
        "        documents.append(tokens)\n",
        "    return documents\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "    # load documents\n",
        "    directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "    directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "\n",
        "    neg = process_docs(directory_neg, vocab, is_train)\n",
        "    pos = process_docs(directory_pos, vocab, is_train)\n",
        "    docs = neg + pos\n",
        "    # prepare labels\n",
        "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "    return docs, labels\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "# integer encode and pad documents\n",
        "def encode_docs(tokenizer, max_length, docs):\n",
        "    # integer encode\n",
        "    encoded = tokenizer.texts_to_sequences(docs)\n",
        "    # pad sequences\n",
        "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "    return padded\n",
        "\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
        "    # clean review\n",
        "    line = clean_doc(review, vocab)\n",
        "    # encode and pad review\n",
        "    padded = encode_docs(tokenizer, max_length, [line])\n",
        "    # predict sentiment\n",
        "    yhat = model.predict(padded, verbose=0)\n",
        "    # retrieve predicted percentage and label\n",
        "    percent_pos = yhat[0,0]\n",
        "    if round(percent_pos) == 0:\n",
        "        return (1-percent_pos), 'NEGATIVE'\n",
        "    return percent_pos, 'POSITIVE'\n",
        "\n",
        "# load the vocabulary\n",
        "vocab_filename = '/content/gdrive/My Drive/txt_sentoken/vocab_train.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "\n",
        "# load all reviews\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('Maximum length: %d' % max_length)\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
        "\n",
        "# load the model\n",
        "model = load_model('/content/gdrive/My Drive/txt_sentoken/model.h5')\n",
        "\n",
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
        "print('Train Accuracy: %.2f' % (acc*100))\n",
        "# evaluate model on test dataset\n",
        "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %.2f' % (acc*100))\n",
        "\n",
        "# test positive text\n",
        "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "\n",
        "# test negative text\n",
        "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXAsWkBkBvVW",
        "outputId": "ea337d35-f4fe-408f-b816-51f87d034899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 25768\n",
            "Maximum length: 1317\n",
            "Train Accuracy: 100.00\n",
            "Test Accuracy: 86.50\n",
            "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
            "Sentiment: NEGATIVE (50.480%)\n",
            "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
            "Sentiment: NEGATIVE (57.071%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test positive text\n",
        "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
        "\n",
        "# test negative text\n",
        "text = 'This is a boring movie. Do not watch it. It sucks.'\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print('\\n\\nReview: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyi9YG_JGqpM",
        "outputId": "1b737232-9070-4500-e996-1679edcbb36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
            "Sentiment: NEGATIVE (50.730%)\n",
            "\n",
            "\n",
            "Review: [This is a boring movie. Do not watch it. It sucks.]\n",
            "Sentiment: NEGATIVE (51.135%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extensions\n",
        "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
        "- **Data Cleaning**. Explore better data cleaning, perhaps leaving some punctuation in tact\n",
        "or normalizing contractions.\n",
        "- **Truncated Sequences**. Padding all sequences to the length of the longest sequence might be extreme if the longest sequence is very di\u000berent to all other reviews. Study the distribution of review lengths and truncate reviews to a mean length.\n",
        "- **Truncated Vocabulary**. We removed infrequently occurring words, but still had a large vocabulary of more than 25,000 words. Explore further reducing the size of the vocabulary and the effect on model skill.\n",
        "- **Filters and Kernel Size**. The number of filters and kernel size are important to model skill and were not tuned. Explore tuning these two CNN parameters.\n",
        "- **Epochs and Batch Size**. The model appears to fit the training dataset quickly. Explore alternate configurations of the number of training epochs and batch size and use the test dataset as a validation set to pick a better stopping point for training the model.\n",
        "- **Deeper Network**. Explore whether a deeper network results in better skill, either in terms of CNN layers, MLP layers and both.\n",
        "- **Pre-Train an Embedding**. Explore pre-training a Word2Vec word embedding in the model and the impact on model skill with and without further fine tuning during training.\n",
        "- **Use GloVe Embedding**. Explore loading the pre-trained GloVe embedding and the impact on model skill with and without further fine tuning during training.\n",
        "- **Longer Test Reviews**. Explore whether the skill of model predictions is dependent on the length of movie reviews as suspected in the final section on evaluating the model.\n",
        "- **Train Final Model**. Train a final model on all available data and use it make predictions on real ad hoc movie reviews from the internet."
      ],
      "metadata": {
        "id": "yxHF9SQBJFZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SSJJUHuvG3vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project: Develop an n-gram CNN Model for Sentiment Analysis**\n",
        "\n",
        "\n",
        "\n",
        "This tutorial is divided into the following parts:\n",
        "1. Movie Review Dataset. *(like previouse section)*\n",
        "2. Data Preparation.    *(like previouse section only X and y are saved in Pickles)*\n",
        "3. Develop Multi-channel Model.\n",
        "4. Evaluate Model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "we want to save the prepared train and test sets to file so that we can load them later for modeling and model evaluation. \n",
        "\n",
        "The function below-named *save_datase()* will save a given prepared dataset (X and y elements) to a file using the pickle API (this is the standard\n",
        "API for saving objects in Python).\n",
        "\n",
        "Running the example cleans the text movie review documents, creates labels, and saves the prepared data for both train and test datasets in train.pkl and test.pkl respectively. \n"
      ],
      "metadata": {
        "id": "NKtFOowJJ_t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "from pickle import dump\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    # remove punctuation from each word\n",
        "    tokens = [re_punc.sub('', w) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens\n",
        "\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, is_train):\n",
        "    documents = list()\n",
        "    # walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "        # skip any reviews in the test set\n",
        "        if is_train and filename.startswith('cv9'):\n",
        "            continue\n",
        "        if not is_train and not filename.startswith('cv9'):\n",
        "            continue\n",
        "        # create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "        # load the doc\n",
        "        doc = load_doc(path)\n",
        "        # clean doc\n",
        "        tokens = clean_doc(doc)\n",
        "        # add to list\n",
        "        documents.append(tokens)\n",
        "    return documents\n",
        "\n",
        "# load and clean a dataset\n",
        "def load_clean_dataset(is_train):\n",
        "    # load documents\n",
        "    directory_pos = '/content/gdrive/My Drive/txt_sentoken/pos'\n",
        "    directory_neg = '/content/gdrive/My Drive/txt_sentoken/neg'\n",
        "\n",
        "    neg = process_docs(directory_neg, is_train)\n",
        "    pos = process_docs(directory_pos, is_train)\n",
        "    docs = neg + pos\n",
        "    # prepare labels\n",
        "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
        "    return docs, labels\n",
        "\n",
        "\n",
        "# save a dataset to file\n",
        "def save_dataset(dataset, filename):\n",
        "    dump(dataset, open(filename, 'wb'))\n",
        "    print('Saved: %s' % filename)\n",
        "\n",
        "# load and clean all reviews\n",
        "train_docs, ytrain = load_clean_dataset(True)\n",
        "test_docs, ytest = load_clean_dataset(False)\n",
        "\n",
        "# save training datasets\n",
        "save_dataset([train_docs, ytrain], '/content/gdrive/My Drive/txt_sentoken/train.pkl')\n",
        "save_dataset([test_docs, ytest], '/content/gdrive/My Drive/txt_sentoken/test.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YrQRMQ8LKj8",
        "outputId": "27e5e46d-a085-4788-cb7b-7311884fcd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/gdrive/My Drive/txt_sentoken/train.pkl\n",
            "Saved: /content/gdrive/My Drive/txt_sentoken/test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Develop Multi-channel Model**\n",
        "\n",
        "In this section, we will develop a multi-channel convolutional neural network for the sentiment analysis prediction problem. This section is divided into 3 parts:\n",
        "1. Encode Data\n",
        "2. Define Model.\n",
        "3. Complete Example.\n",
        "\n",
        "**Encode Data**\n",
        "\n",
        "The first step is to load the cleaned training dataset. The function below-named *load_dataset()* can be called to load the pickled training dataset.\n",
        "\n",
        "Next, we must fit a Keras Tokenizer on the training dataset. We will use this tokenizer to both define the vocabulary for the Embedding layer and encode the review documents as integers.\n",
        "\n",
        "The function *create_tokenizer()* below will create a Tokenizer given a list of documents.\n",
        "\n",
        "We also need to know the maximum length of input sequences as input for the model and to pad all sequences to the fixed length. The function *max_length()* below will calculate the maximum length (number of words) for all reviews in the training dataset.\n",
        "\n",
        "We also need to know the size of the vocabulary for the Embedding layer. This can be calculated from the prepared Tokenizer\n",
        "\n",
        "Finally, we can integer encode and pad the clean movie review text. The function below named *encode_text()* will both encode and pad text data to the maximum review length."
      ],
      "metadata": {
        "id": "bCBBGSFTMoU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Model**\n",
        "\n",
        "A standard model for document classiffication is to use an Embedding layer as input, followed by a one-dimensional convolutional neural network, pooling layer, and then a prediction output layer.\n",
        "\n",
        "The kernel size in the convolutional layer defines the number of words to consider as the convolution is passed across the input text document, providing a grouping parameter. \n",
        "\n",
        "A multi-channel convolutional neural network for document classiffication involves using multiple versions of the standard model with di\u000berent sized kernels. \n",
        "\n",
        "This allows the document to be processed at different resolutions or different *n-grams* (groups of words) at a time, whilst the model learns how to best integrate these interpretations.\n",
        "\n",
        "In Keras, a multiple-input model can be defined using the functional API. We will define a model with three input channels for processing 4-grams, 6-grams, and 8-grams of movie review text. \n",
        "\n",
        "Each channel is comprised of the following elements:\n",
        "- **Input layer** that defines the length of input sequences.\n",
        "- **Embedding layer** set to the size of the vocabulary and 100-dimensional real-valued representations.\n",
        "- **Conv1D layer** with 32 filters and a kernel size set to the number of words to read at once.\n",
        "- **MaxPooling1D** layer to consolidate the output from the convolutional layer.\n",
        "- **Flatten layer** to reduce the three-dimensional output to two dimensional for concatenation.\n",
        "\n",
        "The output from the three channels are concatenated into a single vector and process by a Dense layer and an output layer. \n",
        "\n",
        "The function below defines and returns the model."
      ],
      "metadata": {
        "id": "CXB5UbdWNhgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "    # channel 1\n",
        "    inputs1 = Input(shape=(length,))\n",
        "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "    conv1 = Conv1D(32, 4, activation='relu')(embedding1)\n",
        "    drop1 = Dropout(0.5)(conv1)\n",
        "    pool1 = MaxPooling1D()(drop1)\n",
        "    flat1 = Flatten()(pool1)\n",
        "\n",
        "    # channel 2\n",
        "    inputs2 = Input(shape=(length,))\n",
        "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "    conv2 = Conv1D(32, 6, activation='relu')(embedding2)\n",
        "    drop2 = Dropout(0.5)(conv2)\n",
        "    pool2 = MaxPooling1D()(drop2)\n",
        "    flat2 = Flatten()(pool2)\n",
        "\n",
        "    # channel 3\n",
        "    inputs3 = Input(shape=(length,))\n",
        "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "    conv3 = Conv1D(32, 8, activation='relu')(embedding3)\n",
        "    drop3 = Dropout(0.5)(conv3)\n",
        "    pool3 = MaxPooling1D()(drop3)\n",
        "    flat3 = Flatten()(pool3)\n",
        "    \n",
        "    # merge\n",
        "    merged = concatenate([flat1, flat2, flat3])\n",
        "    # interpretation\n",
        "    dense1 = Dense(10, activation='relu')(merged)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "    # compile\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize\n",
        "    model.summary()\n",
        "    plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    return model"
      ],
      "metadata": {
        "id": "vFVzkp6NLwsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complete Example**\n",
        "\n",
        "Pulling all of this together, the complete example is listed below."
      ],
      "metadata": {
        "id": "2dRtbsgzPVuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "    return max([len(s.split()) for s in lines])\n",
        "\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "    # integer encode\n",
        "    encoded = tokenizer.texts_to_sequences(lines)\n",
        "    # pad encoded sequences\n",
        "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "    return padded\n",
        "\n",
        "# define the model\n",
        "def define_model(length, vocab_size):\n",
        "    # channel 1\n",
        "    inputs1 = Input(shape=(length,))\n",
        "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
        "    conv1 = Conv1D(32, 4, activation='relu')(embedding1)\n",
        "    drop1 = Dropout(0.5)(conv1)\n",
        "    pool1 = MaxPooling1D()(drop1)\n",
        "    flat1 = Flatten()(pool1)\n",
        "\n",
        "    # channel 2\n",
        "    inputs2 = Input(shape=(length,))\n",
        "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
        "    conv2 = Conv1D(32, 6, activation='relu')(embedding2)\n",
        "    drop2 = Dropout(0.5)(conv2)\n",
        "    pool2 = MaxPooling1D()(drop2)\n",
        "    flat2 = Flatten()(pool2)\n",
        "\n",
        "    # channel 3\n",
        "    inputs3 = Input(shape=(length,))\n",
        "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
        "    conv3 = Conv1D(32, 8, activation='relu')(embedding3)\n",
        "    drop3 = Dropout(0.5)(conv3)\n",
        "    pool3 = MaxPooling1D()(drop3)\n",
        "    flat3 = Flatten()(pool3)\n",
        "    \n",
        "    # merge\n",
        "    merged = concatenate([flat1, flat2, flat3])\n",
        "    # interpretation\n",
        "    dense1 = Dense(10, activation='relu')(merged)\n",
        "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
        "    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
        "    # compile\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # summarize\n",
        "    model.summary()\n",
        "    plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
        "    return model\n",
        "\n",
        "# load training dataset\n",
        "trainLines, trainLabels = load_dataset('/content/gdrive/My Drive/txt_sentoken/train.pkl')\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "print('Max document length: %d' % length)\n",
        "\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "# define model\n",
        "model = define_model(length, vocab_size)\n",
        "# fit model\n",
        "model.fit([trainX,trainX,trainX], array(trainLabels), epochs=7, batch_size=16)\n",
        "# save the model\n",
        "model.save('/content/gdrive/My Drive/txt_sentoken/model_1.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAaz9NP4PR5V",
        "outputId": "7329a9e0-2fab-41ab-ce45-db190483fbb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max document length: 1380\n",
            "Vocabulary size: 44277\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 1380)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 1380)]       0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 1380)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 1380, 100)    4427700     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)        (None, 1380, 100)    4427700     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_3 (Embedding)        (None, 1380, 100)    4427700     ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 1377, 32)     12832       ['embedding_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 1375, 32)     19232       ['embedding_2[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 1373, 32)     25632       ['embedding_3[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 1377, 32)     0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 1375, 32)     0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 1373, 32)     0           ['conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 688, 32)     0           ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 687, 32)     0           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 686, 32)     0           ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 22016)        0           ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)            (None, 21984)        0           ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_3 (Flatten)            (None, 21952)        0           ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 65952)        0           ['flatten_1[0][0]',              \n",
            "                                                                  'flatten_2[0][0]',              \n",
            "                                                                  'flatten_3[0][0]']              \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 10)           659530      ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            11          ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 14,000,337\n",
            "Trainable params: 14,000,337\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/7\n",
            "113/113 [==============================] - 3s 20ms/step - loss: 0.6951 - accuracy: 0.5406\n",
            "Epoch 2/7\n",
            "113/113 [==============================] - 2s 20ms/step - loss: 0.2675 - accuracy: 0.9372\n",
            "Epoch 3/7\n",
            "113/113 [==============================] - 2s 20ms/step - loss: 0.0100 - accuracy: 0.9989\n",
            "Epoch 4/7\n",
            "113/113 [==============================] - 2s 19ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 5/7\n",
            "113/113 [==============================] - 2s 20ms/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 6/7\n",
            "113/113 [==============================] - 2s 20ms/step - loss: 5.7147e-04 - accuracy: 1.0000\n",
            "Epoch 7/7\n",
            "113/113 [==============================] - 2s 20ms/step - loss: 4.2399e-04 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ctTA4_B2PbF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Model**\n",
        "\n",
        "In this section, we can evaluate the fit model by predicting the sentiment on all reviews in the unseen test dataset. \n",
        "\n",
        "Using the data loading functions developed in the previous section, we can\n",
        "load and encode both the training and test datasets.\n",
        "\n",
        "We can load the saved model and evaluate it on both the training and test datasets. \n",
        "\n",
        "The complete example is listed below."
      ],
      "metadata": {
        "id": "o29xf1J-QcrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "# load a clean dataset\n",
        "def load_dataset(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "# calculate the maximum document length\n",
        "def max_length(lines):\n",
        "    return max([len(s.split()) for s in lines])\n",
        "# encode a list of lines\n",
        "def encode_text(tokenizer, lines, length):\n",
        "    # integer encode\n",
        "    encoded = tokenizer.texts_to_sequences(lines)\n",
        "    # pad encoded sequences\n",
        "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
        "    return padded\n",
        "    \n",
        "# load datasets\n",
        "trainLines, trainLabels = load_dataset('/content/gdrive/My Drive/txt_sentoken/train.pkl')\n",
        "testLines, testLabels = load_dataset('/content/gdrive/My Drive/txt_sentoken/test.pkl')\n",
        "# create tokenizer\n",
        "tokenizer = create_tokenizer(trainLines)\n",
        "# calculate max document length\n",
        "length = max_length(trainLines)\n",
        "print('Max document length: %d' % length)\n",
        "# calculate vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary size: %d' % vocab_size)\n",
        "# encode data\n",
        "trainX = encode_text(tokenizer, trainLines, length)\n",
        "testX = encode_text(tokenizer, testLines, length)\n",
        "\n",
        "# load the model\n",
        "model = load_model('/content/gdrive/My Drive/txt_sentoken/model_1.h5')\n",
        "# evaluate model on training dataset\n",
        "_, acc = model.evaluate([trainX,trainX,trainX], array(trainLabels), verbose=0)\n",
        "print('Train Accuracy: %.2f' % (acc*100))\n",
        "# evaluate model on test dataset dataset\n",
        "_, acc = model.evaluate([testX,testX,testX], array(testLabels), verbose=0)\n",
        "print('Test Accuracy: %.2f' % (acc*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6QdnqGjQu4B",
        "outputId": "7ef4973a-7ff4-4fb0-df2f-866552391e46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max document length: 1380\n",
            "Vocabulary size: 44277\n",
            "Train Accuracy: 100.00\n",
            "Test Accuracy: 81.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "amXtRvv3RKsK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}